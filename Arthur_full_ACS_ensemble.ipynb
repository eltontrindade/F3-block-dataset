{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Visualização do dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "import src\n",
    "import make_dataset_ensemble\n",
    "make_dataset_ensemble.main(n_wells=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**2. Data preparation**](02-data-preparation.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial 2D training\n",
    "\n",
    "import train2d_ensemble\n",
    "train2d_ensemble.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2,5D Training for each 2D view of the data\n",
    "import train25d_ensemble\n",
    "for i in range(3):\n",
    "\n",
    "    train25d_ensemble.main(canal=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Treino dos canais num ensemble em 2,5D, sendo a concatenação apenas antes da última convolução 3D\n",
    "import MyEnsemble25d_train\n",
    "MyEnsemble25d_train.main(canal=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Treino dos canais num ensemble em 2,5D, sendo a concatenação seguida de outra convolução antes da última convolução 3D\n",
    "import MyEnsemble25d_trainv2\n",
    "MyEnsemble25d_trainv2.main(canal=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D UNET RANDOM INITIALIZED\n",
    "\n",
    "\n",
    "import _init_paths\n",
    "\n",
    "import fire\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import sklearn\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from poc_dataset_ACS import BaseDatasetVoxel\n",
    "from mylib.loss import soft_cross_entropy_loss\n",
    "from mylib.utils import MultiAverageMeter, save_model, log_results, to_var, set_seed, \\\n",
    "        to_device, initialize, categorical_to_one_hot, copy_file_backup, redirect_stdout\n",
    "from poc_config_3D import POCVoxelEnv as env\n",
    "from poc_config_3D import POCVoxelConfig as cfg\n",
    "\n",
    "from unet import UNet\n",
    "from acsconv.models import ACSUNet\n",
    "from acsconv.converters import ACSConverter, Conv3dConverter, Conv2_5dConverter\n",
    "\n",
    "from mylib.metrics import cal_batch_iou, cal_batch_dice\n",
    "from mylib.loss import soft_dice_loss\n",
    "import timeit\n",
    "def main(save_path=cfg.save, \n",
    "         n_epochs=cfg.n_epochs, \n",
    "         seed=cfg.seed\n",
    "         ):\n",
    "    start_time = timeit.default_timer()\n",
    "    if seed is not None:\n",
    "        set_seed(cfg.seed)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "\n",
    "\n",
    "    # # Models\n",
    "    os.makedirs(save_path,exist_ok = True)\n",
    "    #copy_file_backup(save_path)\n",
    "    redirect_stdout(save_path)\n",
    "\n",
    "    # Datasets\n",
    "    train_data = env.data_train\n",
    "    test_data = env.data_test\n",
    "    shape_cp = env.shape_checkpoint\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    train_set = BaseDatasetVoxel(train_data, cfg.train_samples)\n",
    "    valid_set = None\n",
    "    test_set = BaseDatasetVoxel(test_data, cfg.test_samples)\n",
    "\n",
    "    model = UNet(6)\n",
    "    if cfg.conv == 'Conv3D':\n",
    "        model = Conv3dConverter(model)\n",
    "        initialize(model.modules())\n",
    "    elif cfg.conv == 'Conv2_5D':\n",
    "        print('REDE 2,5D')\n",
    "        if cfg.pretrained:\n",
    "            shape_cp = torch.load(shape_cp)\n",
    "            shape_cp.popitem()\n",
    "            shape_cp.popitem()\n",
    "            incompatible_keys = model.load_state_dict(shape_cp, strict=False)\n",
    "            print('load shape pretrained weights\\n', incompatible_keys)\n",
    "        model = Conv2_5dConverter(model)\n",
    "    elif cfg.conv == 'ACSConv':\n",
    "        # You can use either the naive ``ACSUNet`` or the ``ACSConverter(model)``\n",
    "        #model = ACSConverter(model)\n",
    "        model = ACSUNet(4)\n",
    "        if cfg.pretrained:\n",
    "            shape_cp = torch.load(shape_cp)\n",
    "            shape_cp.popitem()\n",
    "            shape_cp.popitem()\n",
    "            incompatible_keys = model.load_state_dict(shape_cp, strict=False)\n",
    "            print('load shape pretrained weights\\n', incompatible_keys)\n",
    "    else:\n",
    "        raise ValueError('not valid conv')\n",
    "    \n",
    "    print(model)\n",
    "    torch.save(model.state_dict(), os.path.join(save_path, 'model.dat'))\n",
    "    # Train the model\n",
    "    train(model=model, train_set=train_set, valid_set=valid_set, test_set=test_set, save=save_path, n_epochs=n_epochs)\n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "    print(elapsed)\n",
    "    print('Done!')\n",
    "    \n",
    "    \n",
    "\n",
    "def train(model, train_set, test_set, save, valid_set, n_epochs):\n",
    "\n",
    "\n",
    "    # Data loaders\n",
    "    train_loader = DataLoader(train_set, batch_size=cfg.train_batch_size, shuffle=True,\n",
    "                                pin_memory=(torch.cuda.is_available()), num_workers=cfg.num_workers)\n",
    "    test_loader = DataLoader(test_set, batch_size=cfg.test_batch_size, shuffle=False,\n",
    "                                pin_memory=(torch.cuda.is_available()), num_workers=cfg.num_workers)\n",
    "    if valid_set is None:\n",
    "        valid_loader = None\n",
    "    else:\n",
    "        valid_loader = DataLoader(valid_set, batch_size=cfg.batch_size, shuffle=False,\n",
    "                                pin_memory=(torch.cuda.is_available()), num_workers=cfg.num_workers)\n",
    "    # Model on cuda\n",
    "    model = to_device(model)\n",
    "\n",
    "    # Wrap model for multi-GPUs, if necessary\n",
    "    model_wrapper = model\n",
    "    if torch.cuda.is_available() and torch.cuda.device_count() > 1:\n",
    "        model_wrapper = torch.nn.DataParallel(model).cuda()\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.Adam(model_wrapper.parameters(), lr=cfg.lr)\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=cfg.milestones,\n",
    "                                                     gamma=cfg.gamma)\n",
    "\n",
    "    # Start log\n",
    "    logs = ['loss', 'iou', 'dice'] + ['iou{}'.format(i) for i in range(6)]+['dice{}'.format(i) for i in range(6)]\n",
    "    train_logs = ['train_'+log for log in logs]\n",
    "    test_logs = ['test_'+log for log in logs]\n",
    "    log_dict = OrderedDict.fromkeys(train_logs+test_logs, 0)\n",
    "    with open(os.path.join(save, 'logs.csv'), 'w') as f:\n",
    "        f.write('epoch,')\n",
    "        for key in log_dict.keys():\n",
    "            f.write(key+',')\n",
    "        f.write('\\n')\n",
    "    writer = SummaryWriter(log_dir=os.path.join(save, 'Tensorboard_Results'))\n",
    "\n",
    "    # Train model\n",
    "    best_dice = 0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        os.makedirs(os.path.join(cfg.save, 'epoch_{}'.format(epoch)),exist_ok = True)\n",
    "        train_meters = train_epoch(\n",
    "            model=model_wrapper,\n",
    "            loader=train_loader,\n",
    "            optimizer=optimizer,\n",
    "            epoch=epoch,\n",
    "            n_epochs=n_epochs,\n",
    "            writer=writer\n",
    "        )\n",
    "        # if (epoch+1)%5==0:\n",
    "        test_meters = test_epoch(\n",
    "            model=model_wrapper,\n",
    "            loader=test_loader,\n",
    "            epoch=epoch,\n",
    "            is_test=True,\n",
    "            writer = writer\n",
    "        )\n",
    "        scheduler.step()\n",
    "\n",
    "        # Log results\n",
    "        for i, key in enumerate(train_logs):\n",
    "            log_dict[key] = train_meters[i]\n",
    "        for i, key in enumerate(test_logs):\n",
    "            log_dict[key] = test_meters[i]\n",
    "\n",
    "        log_results(save, epoch, log_dict, writer=writer)\n",
    "\n",
    "        if cfg.save_all:\n",
    "            torch.save(model.state_dict(), os.path.join(save, 'epoch_{}'.format(epoch), 'model.dat'))\n",
    "\n",
    "        if log_dict['test_dice'] > best_dice:\n",
    "            torch.save(model.state_dict(), os.path.join(save, 'model.dat'))\n",
    "            best_dice = log_dict['test_dice']\n",
    "            print('New best dice: %.4f' % log_dict['test_dice'])\n",
    "            #print(2.*intersection/union)\n",
    "        else:\n",
    "            print('Current best dice: %.4f' % best_dice)\n",
    "            #print(2.*intersection/union)\n",
    "    writer.close()\n",
    "\n",
    "    with open(os.path.join(save, 'logs.csv'), 'a') as f:\n",
    "        f.write(',,,,best dice,%0.5f\\n' % (best_dice))\n",
    "    # Final test of the best model on test set\n",
    "    print('best dice: ', best_dice)\n",
    "\n",
    "iteration = 0\n",
    "\n",
    "def train_epoch(model, loader, optimizer, epoch, n_epochs, print_freq=1, writer=None):\n",
    "    meters = MultiAverageMeter()\n",
    "    # Model on train mode\n",
    "    model.train()\n",
    "    global iteration\n",
    "    end = time.time()\n",
    "    for batch_idx, (x, y) in enumerate(loader):\n",
    "        # Create vaiables\n",
    "        x = to_var(x)\n",
    "        y = to_var(y)\n",
    "        # compute output\n",
    "        pred_logit = model(x)\n",
    "        loss = soft_dice_loss(pred_logit, y, smooth=1e-2)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        y = y.long()\n",
    "        \n",
    "        batch_size = y.size(0)\n",
    "        iou = cal_batch_iou(pred_logit, y)\n",
    "        dice = cal_batch_dice(pred_logit, y)\n",
    "        \n",
    "\n",
    "        logs = [loss.item(), iou[1:].mean(), dice[1:].mean()]+ \\\n",
    "                            [iou[i].item() for i in range(len(iou))]+ \\\n",
    "                            [dice[i].item() for i in range(len(dice))]+ \\\n",
    "                            [time.time() - end]\n",
    "        meters.update(logs, batch_size)   \n",
    "        writer.add_scalar('train_loss_logs', loss.item(), iteration)\n",
    "        with open(os.path.join(cfg.save, 'loss_logs.csv'), 'a') as f:\n",
    "            f.write('%09d,%0.6f,\\n'%((iteration + 1),loss.item(),))\n",
    "        iteration += 1\n",
    "\n",
    "\n",
    "        # measure elapsed time\n",
    "        end = time.time()\n",
    "        # print stats\n",
    "        print_freq = 2 // meters.val[-1] + 1\n",
    "        if batch_idx % print_freq == 0:\n",
    "            res = '\\t'.join([\n",
    "                'Epoch: [%d/%d]' % (epoch + 1, n_epochs),\n",
    "                'Iter: [%d/%d]' % (batch_idx + 1, len(loader)),\n",
    "                'Time %.3f (%.3f)' % (meters.val[-1], meters.avg[-1]),\n",
    "                'Loss %.4f (%.4f)' % (meters.val[0], meters.avg[0]),\n",
    "                'IOU %.4f (%.4f)' % (meters.val[1], meters.avg[1]),\n",
    "                'DICE %.4f (%.4f)' % (meters.val[2], meters.avg[2]),\n",
    "            ])\n",
    "            print(res)\n",
    "\n",
    "    return meters.avg[:-1] #intersection, union\n",
    "\n",
    "\n",
    "def test_epoch(model, loader, epoch, print_freq=1, is_test=True, writer=None):\n",
    "    meters = MultiAverageMeter()\n",
    "    # Model on eval mode\n",
    "    model.eval()\n",
    "    gt_classes = []\n",
    "    pred_all_probs = []\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (x, y) in enumerate(loader):\n",
    "            \n",
    "            x = to_var(x)\n",
    "            \n",
    "            \n",
    "            y = to_var(y)\n",
    "            \n",
    "            pred_logit = model(x)\n",
    "            \n",
    "            # calculate metrics\n",
    "            pred_class = pred_logit.max(dim=1)[1]\n",
    "            pred_probs = pred_logit.softmax(-1)\n",
    "            pred_all_probs.append(pred_probs.cpu())\n",
    "            gt_classes.append(y.cpu())\n",
    "            \n",
    "            #print(gt_classes.shape) #pred_class[20,48,48,48]\n",
    "            #print(pred_probs[1]) #y e pred_probs[20,6,48,48,48]\n",
    "            \n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "            loss = soft_dice_loss(pred_logit, y, smooth=1e-2)\n",
    "            y = y.long()\n",
    "            batch_size = y.size(0)\n",
    "            iou = cal_batch_iou(pred_logit, y)\n",
    "            \n",
    "            dice = cal_batch_dice(pred_logit, y)\n",
    "            \n",
    "            \n",
    "            logs = [loss.item(), iou[1:].mean(), dice[1:].mean()]+ \\\n",
    "                                [iou[i].item() for i in range(len(iou))]+ \\\n",
    "                                [dice[i].item() for i in range(len(dice))]+ \\\n",
    "                                [time.time() - end]\n",
    "            meters.update(logs, batch_size)\n",
    "\n",
    "            end = time.time()\n",
    "\n",
    "            print_freq = 2 // meters.val[-1] + 1\n",
    "            if batch_idx % print_freq == 0:\n",
    "                res = '\\t'.join([\n",
    "                    'Test' if is_test else 'Valid',\n",
    "                    'Iter: [%d/%d]' % (batch_idx + 1, len(loader)),\n",
    "                    'Time %.3f (%.3f)' % (meters.val[-1], meters.avg[-1]),\n",
    "                    'Loss %.4f (%.4f)' % (meters.val[0], meters.avg[0]),\n",
    "                    'IOU %.4f (%.4f)' % (meters.val[1], meters.avg[1]),\n",
    "                    'DICE %.4f (%.4f)' % (meters.val[2], meters.avg[2]),\n",
    "                ])\n",
    "                print(res)\n",
    "                #print(meters.avg[:-1])\n",
    "\n",
    "    return meters.avg[:-1]\n",
    "\n",
    "main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
