{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Visualização do dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x={5}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "import src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import make_dataset\n",
    "\n",
    "make_dataset.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Informações do dataset\n",
    "\n",
    "- Número de exemplos de treinamento e teste\n",
    "- Shape dos exemplos do dataset\n",
    "- Distribuições de frequência do conjunto de treinamento e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filename = r'C:\\Users\\admin\\Desktop\\cnn-facies-classifier-master\\data\\interim\\stanford6_32.h5'\n",
    "with h5py.File(filename, 'r') as dataset:\n",
    "    x_train_original = np.array(dataset['train/X'])\n",
    "    y_train_original = np.array(dataset['train/Y'])\n",
    "    x_test_original = np.array(dataset['test/X'])\n",
    "    y_test_original = np.array(dataset['test/Y'])\n",
    "\n",
    "classnames = {\n",
    "    0: 'Floodplain',\n",
    "    1: 'Pointbar',\n",
    "    2: 'Channel',\n",
    "    3: 'Boundary',\n",
    "}    \n",
    "    \n",
    "m = x_train_original.shape[0]\n",
    "num_classes = 4\n",
    "\n",
    "train_freq = src.class_frequency(y_train_original, num_classes)\n",
    "test_freq = src.class_frequency(y_test_original, num_classes)\n",
    "\n",
    "print(f\"Training examples: {y_train_original.shape[0]}\")\n",
    "print(f\"Test examples: {y_test_original.shape[0]}\")\n",
    "print(f\"Example shape: {x_train_original.shape[1:]}\")\n",
    "\n",
    "src.plot_classes_freq(train_freq, classnames.values(), title='Training classes')\n",
    "src.plot_classes_freq(test_freq, classnames.values(), title='Test classes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualização de 10 imagens de treinamento aleatórias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows, ncols = 2, 5\n",
    "idx = np.random.choice(m, nrows * ncols)\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "for i in range(1, nrows * ncols +1):\n",
    "    fig.add_subplot(nrows, ncols, i)\n",
    "    plt.imshow(x_train_original[idx[i-1]])\n",
    "    plt.title(f'class: {classnames[y_train_original[idx[i-1]][0]]}')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**2. Data preparation**](02-data-preparation.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import h5py\n",
    "import src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import h5py\n",
    "import keras\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = r'C:\\Users\\admin\\Desktop\\cnn-facies-classifier-master\\data\\interim\\stanford6_32.h5'\n",
    "with h5py.File(filename, 'r') as dataset:\n",
    "    x_train_original = np.array(dataset['train/X'])\n",
    "    y_train_original = np.array(dataset['train/Y'])\n",
    "    x_test_original = np.array(dataset['test/X'])\n",
    "    y_test_original = np.array(dataset['test/Y'])\n",
    "\n",
    "\n",
    "classnames = {\n",
    "    0: 'Floodplain',\n",
    "    1: 'Pointbar',\n",
    "    2: 'Channel',\n",
    "    3: 'Boundary',\n",
    "}        \n",
    "\n",
    "m = x_train_original.shape[0]\n",
    "num_classes = 4\n",
    "\n",
    "resampler = RandomOverSampler()\n",
    "\n",
    "x_train_resampled, y_train_resampled = resampler.fit_resample(\n",
    "    np.reshape(x_train_original, (m, np.product(x_train_original.shape[1:]))), \n",
    "    y_train_original\n",
    ")\n",
    "x_train_resampled = np.reshape(\n",
    "    x_train_resampled, \n",
    "    (x_train_resampled.shape[0], *x_train_original.shape[1:])\n",
    ")\n",
    "\n",
    "#tentar puxar so cubo original os labels das coordenadas após o balanceamento\n",
    "\n",
    "src.plot_classes_freq(\n",
    "    src.class_frequency(y_train_resampled, num_classes),\n",
    "    classnames.values(),\n",
    "    title='Resampled training set'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "m = x_train_resampled.shape[0]\n",
    "idx = np.random.choice(m, int(m * 0.2))\n",
    "mask = np.ones(m, dtype=bool)\n",
    "mask[idx] = False\n",
    "\n",
    "x_train_split, x_val_split = x_train_resampled[mask], x_train_resampled[idx]\n",
    "y_train_split, y_val_split = y_train_resampled[mask], y_train_resampled[idx]\n",
    "\n",
    "m = x_train_original.shape[0]\n",
    "idx = np.random.choice(m, int(m * 0.2))\n",
    "mask = np.ones(m, dtype=bool)\n",
    "mask[idx] = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "src.plot_classes_freq(\n",
    "    src.class_frequency(y_train_split, num_classes),\n",
    "    classnames.values(),\n",
    "    title='Final training set'\n",
    ")\n",
    "\n",
    "src.plot_classes_freq(\n",
    "    src.class_frequency(y_val_split, num_classes),\n",
    "    classnames.values(),\n",
    "    title='Final validation set'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x_train.shape,x_val.shape]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sem balanceamento\n",
    "\n",
    "np.random.seed(22)\n",
    "\n",
    "m = x_train_original.shape[0]\n",
    "idx = np.random.choice(m, int(m * 0.2))\n",
    "mask = np.ones(m, dtype=bool)\n",
    "mask[idx] = False\n",
    "\n",
    "x_train_split, x_val_split = x_train_original, x_train_original\n",
    "y_train_split, y_val_split = y_train_original, y_train_original\n",
    "\n",
    "#x_train_split, x_val_split = x_train_resampled[mask], x_train_resampled[idx]\n",
    "#y_train_split, y_val_split = y_train_resampled[mask], y_train_resampled[idx]\n",
    "\n",
    "\n",
    "x_train = x_train_split.astype('float16') / 255\n",
    "y_train = keras.utils.to_categorical(y_train_split, num_classes)\n",
    "\n",
    "x_val = x_val_split.astype('float16') / 255\n",
    "y_val= keras.utils.to_categorical(y_val_split, num_classes)\n",
    "\n",
    "x_test = x_test_original.astype('float16') / 255\n",
    "y_test = keras.utils.to_categorical(y_test_original, num_classes)\n",
    "\n",
    "out_filename = r'C:\\Users\\admin\\Desktop\\cnn-facies-classifier-master\\data\\processed\\stanford6_32.h5'\n",
    "\n",
    "with h5py.File(out_filename, 'w') as file:\n",
    "    file.create_dataset('test/X', data=x_test)\n",
    "    file.create_dataset('test/Y', data=y_test)\n",
    "    \n",
    "    file.create_dataset('val/X', data=x_val)\n",
    "    file.create_dataset('val/Y', data=y_val)\n",
    "    \n",
    "    file.create_dataset('train/X', data=x_train)\n",
    "    file.create_dataset('train/Y', data=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Treinamento da rede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import h5py\n",
    "import keras\n",
    "import k3d\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "\n",
    "from livelossplot.keras import PlotLossesCallback\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from ipywidgets import interact, widgets\n",
    "\n",
    "import src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('data\\\\processed\\stanford6_32.h5', 'r') as dataset:\n",
    "    x_train = np.array(dataset['train/X'])\n",
    "    y_train = np.array(dataset['train/Y'])\n",
    "    x_val = np.array(dataset['val/X'])\n",
    "    y_val = np.array(dataset['val/Y'])\n",
    "\n",
    "classnames = {\n",
    "    0: 'Floodplain',\n",
    "    1: 'Pointbar',\n",
    "    2: 'Channel',\n",
    "    3: 'Boundary',\n",
    "}    \n",
    "    \n",
    "m = x_train.shape[0]\n",
    "num_classes = 4\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "input_shape = x_train.shape[1:]\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (7, 7), padding='same', input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(64, (5, 5), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(200))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=0.1, decay=0.0, amsgrad=False)\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=opt,\n",
    "    metrics=['acc', f1_m,precision_m, recall_m]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import timeit\n",
    "batch_size = 32\n",
    "epochs = 1000\n",
    "start_time = timeit.default_timer()\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    validation_data=(x_val, y_val),\n",
    "    epochs=epochs,\n",
    "    shuffle=True,\n",
    "    callbacks=[PlotLossesCallback()],\n",
    ")\n",
    "elapsed = timeit.default_timer() - start_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(r'C:\\Users\\admin\\Desktop\\cnn-facies-classifier-master/models/trained_model_32.h5')\n",
    "print('Model saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTE DO MODELO\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import h5py\n",
    "import keras\n",
    "import k3d\n",
    "from sklearn.metrics import confusion_matrix,jaccard_score, f1_score, precision_score, recall_score\n",
    "from ipywidgets import interact, widgets\n",
    "\n",
    "import src\n",
    "with h5py.File(r'C:\\Users\\admin\\Desktop\\cnn-facies-classifier-master\\data\\processed\\stanford6_32.h5', 'r') as dataset:\n",
    "    x_test = np.array(dataset['test/X'])\n",
    "    y_test = np.array(dataset['test/Y'])\n",
    "\n",
    "classnames = {\n",
    "    0: 'Floodplain',\n",
    "    1: 'Pointbar',\n",
    "    2: 'Channel',\n",
    "    3: 'Boundary',\n",
    "}\n",
    "\n",
    "model = keras.models.load_model(r'C:\\Users\\admin\\Desktop\\cnn-facies-classifier-master\\models/trained_model_32.h5', custom_objects={\"f1_m\":f1_m,\"precision_m\":precision_m,\"recall_m\": recall_m})\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "print('Evaluating model...\\n')\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "y_pred = np.argmax(model.predict(x_test), axis=-1)\n",
    "y_true = np.argmax(y_test, axis=-1)\n",
    "\n",
    "precision = precision_score(y_true, y_pred, average='weighted')\n",
    "recall = recall_score(y_true, y_pred, average='weighted')\n",
    "f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "jaccard=jaccard_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "\n",
    "matrix = confusion_matrix(y_true, y_pred)\n",
    "matrix = matrix.astype('float') / matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "print(f'Precision: \\t{precision}')\n",
    "print(f'Recall: \\t{recall}')\n",
    "print(f'F1-Score: \\t{f1}')\n",
    "print(f'IOU: \\t{jaccard}')\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print(f'elapsed:  \\t {elapsed}')\n",
    "\n",
    "src.plot_confusion_matrix(matrix, classnames.values(), title=\"Confusion matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 32\n",
    "output_shape = [150 + 1 - image_size, 200 + 1 - image_size, 119 + 1 - image_size]\n",
    "\n",
    "def plot_section(z=0):\n",
    "    plt.subplot(121)\n",
    "    plt.imshow(y_true.reshape(output_shape)[:,:,z].T)\n",
    "    plt.subplot(122)\n",
    "    plt.imshow(y_pred.reshape(output_shape)[:,:,z].T)\n",
    "\n",
    "interact(plot_section, z=widgets.IntSlider(min=0,max=119 - image_size,step=1,value=0))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_map = (0x3A528B, 0x20908C, 0xFDE724)\n",
    "\n",
    "plot = k3d.plot(camera_auto_fit=False)\n",
    "obj = k3d.voxels(y_true.reshape(output_shape).T, color_map, compression_level=1)\n",
    "plot += obj\n",
    "plot.camera=[150, 230, -40, 60, 85, 80, 0.0, 0.0, -1.0]\n",
    "plot.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data= np.load(r'C:/Users/admin/Desktop/cnn-facies-classifier-master/data/interim/trainACS.npz')\n",
    "shape = data['shape']\n",
    "segs = data['segs']\n",
    "#segs=np.rollaxis(segs, -1, 1)\n",
    "#segs = segs\n",
    "segs[1]\n",
    "\n",
    "train_set = BaseDatasetShape(env.ACS_train, x_train_ACS.shape[0])\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=cfg.batch_size, shuffle=True,\n",
    "                            pin_memory=(torch.cuda.is_available()), num_workers=cfg.num_workers)\n",
    "\n",
    "\n",
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# encoding: utf-8\n",
    "#Train2D\n",
    "\n",
    "import _init_paths\n",
    "import shutil\n",
    "import fire\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from sklearn import metrics as met\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "from tensorboardX import SummaryWriter\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from unet import UNet\n",
    "from poc_dataset_ACS import BaseDatasetShape\n",
    "from mylib.loss import soft_dice_loss\n",
    "from mylib.utils import MultiAverageMeter, save_model, log_results, to_var, set_seed, \\\n",
    "        to_device, initialize, categorical_to_one_hot, copy_file_backup, redirect_stdout\n",
    "from mylib.metrics import cal_batch_iou, cal_batch_dice, AUROC_per_case\n",
    "from mylib.loss import soft_dice_loss\n",
    "from poc_config import POCShapeConfig as cfg\n",
    "from poc_config import POCShapeEnv as env\n",
    "from livelossplot import PlotLosses\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.use('TKAgg',warn=False, force=True)\n",
    "\n",
    "import timeit\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "\n",
    "grafico={}\n",
    "liveloss = PlotLosses()\n",
    "def main(save_path=cfg.save, \n",
    "         batch_size=cfg.batch_size, \n",
    "         n_epochs=cfg.n_epochs, \n",
    "         drop_rate=cfg.drop_rate,\n",
    "         seed=cfg.seed\n",
    "         ):\n",
    "    start_time = timeit.default_timer()\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(cfg.seed)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    #if os.path.exists(save_path):\n",
    "        #shutil.rmtree(save_path,ignore_errors=True)\n",
    "    os.makedirs(save_path,exist_ok = True)\n",
    "    #copy_file_backup(save_path)\n",
    "    redirect_stdout(save_path)\n",
    "\n",
    "    # Datasets\n",
    "    train_data = env.data_train\n",
    "    test_data = env.data_test\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    train_set = BaseDatasetShape(train_data, cfg.train_samples)\n",
    "    valid_set = None\n",
    "    test_set = BaseDatasetShape(test_data, cfg.test_samples)\n",
    "\n",
    "\n",
    "    # Models\n",
    "    model = UNet(4) #3 é o numero de classes\n",
    "    #model= UNet(n_channels=3, n_classes=1, bilinear=True)\n",
    "    initialize(model.modules())\n",
    "\n",
    "    # Train the model\n",
    "    train(model=model, train_set=train_set, valid_set=valid_set, test_set=test_set, save=save_path, n_epochs=n_epochs)\n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "    print(elapsed)\n",
    "    print('Done!')\n",
    "\n",
    "\n",
    "def train(model, train_set, test_set, save, valid_set, n_epochs):\n",
    "    \n",
    "    \n",
    "\n",
    "    # Data loaders\n",
    "    train_loader = DataLoader(train_set, batch_size=cfg.batch_size, shuffle=True,\n",
    "                                pin_memory=(torch.cuda.is_available()), num_workers=cfg.num_workers)\n",
    "    test_loader = DataLoader(test_set, batch_size=cfg.batch_size, shuffle=False,\n",
    "                                pin_memory=(torch.cuda.is_available()), num_workers=cfg.num_workers)\n",
    "    if valid_set is None:\n",
    "        valid_loader = None\n",
    "    else:\n",
    "        valid_loader = DataLoader(valid_set, batch_size=cfg.batch_size, shuffle=False,\n",
    "                                pin_memory=(torch.cuda.is_available()), num_workers=cfg.num_workers)\n",
    "    # Model on cuda\n",
    "    model = to_device(model)\n",
    "\n",
    "    # Wrap model for multi-GPUs, if necessary\n",
    "    model_wrapper = model\n",
    "    if torch.cuda.is_available() and torch.cuda.device_count() > 1:\n",
    "        model_wrapper = torch.nn.DataParallel(model).cuda()\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.SGD(model_wrapper.parameters(), lr=cfg.lr, weight_decay=cfg.wd, momentum=cfg.momentum)\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=cfg.milestones,\n",
    "                                                     gamma=cfg.gamma)\n",
    "\n",
    "    # Start log\n",
    "    logs = ['loss', 'iou', 'dice'] + ['iou{}'.format(i) for i in range(2)]+['dice{}'.format(i) for i in range(2)]\n",
    "    train_logs = ['train_'+log for log in logs]\n",
    "    test_logs = ['test_'+log for log in logs]\n",
    "    log_dict = OrderedDict.fromkeys(train_logs+test_logs, 0)\n",
    "    with open(os.path.join(save, 'logs.csv'), 'w') as f:\n",
    "        f.write('epoch,')\n",
    "        for key in log_dict.keys():\n",
    "            f.write(key+',')\n",
    "        f.write('\\n')\n",
    "    writer = SummaryWriter(log_dir=os.path.join(save, 'Tensorboard_Results'))\n",
    "\n",
    "    # Train model\n",
    "    best_dice = 0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        os.makedirs(os.path.join(cfg.save, 'epoch_{}'.format(epoch)),exist_ok = True)\n",
    "        grafico={}\n",
    "        train_meters = train_epoch(\n",
    "            model=model_wrapper,\n",
    "            loader=train_loader,\n",
    "            optimizer=optimizer,\n",
    "            epoch=epoch,\n",
    "            n_epochs=n_epochs,\n",
    "            writer=writer\n",
    "        )\n",
    "\n",
    "        \n",
    "        \n",
    "        test_meters = test_epoch(\n",
    "            model=model_wrapper,\n",
    "            loader=test_loader,\n",
    "            epoch=epoch,\n",
    "            is_test=True,\n",
    "            writer = writer\n",
    "        )\n",
    "        scheduler.step()\n",
    "\n",
    "        # Log results\n",
    "        for i, key in enumerate(train_logs):\n",
    "            log_dict[key] = train_meters[i]\n",
    "            \n",
    "        for i, key in enumerate(test_logs):\n",
    "            log_dict[key] = test_meters[i]\n",
    "\n",
    "        log_results(save, epoch, log_dict, writer=writer)\n",
    "        \n",
    "        grafico['log loss'] = log_dict['train_loss']\n",
    "        grafico['dice'] = log_dict['train_dice']  \n",
    "\n",
    "        \n",
    "        \n",
    "        if cfg.save_all:\n",
    "            torch.save(model.state_dict(), os.path.join(save, 'epoch_{}'.format(epoch), 'model.dat'))\n",
    "        if log_dict['test_dice'] > best_dice:\n",
    "            torch.save(model.state_dict(), os.path.join(save, 'model.dat'))\n",
    "            best_dice = log_dict['test_dice']\n",
    "            print('New best dice: %.4f' % log_dict['test_dice'])\n",
    "            \n",
    "        else:\n",
    "            print('Current best dice: %.4f' % best_dice)\n",
    "    \n",
    "    liveloss.update(grafico)\n",
    "    liveloss.send()\n",
    "    writer.close()\n",
    "\n",
    "\n",
    "    with open(os.path.join(save, 'logs.csv'), 'a') as f:\n",
    "        f.write(',,,,best dice,%0.5f\\n' % (best_dice))\n",
    "    # Final test of the best model on test set\n",
    "    print('best dice: ', best_dice)\n",
    "\n",
    "iteration = 0\n",
    "\n",
    "\n",
    "def train_epoch(model, loader, optimizer, epoch, n_epochs, print_freq=1, writer=None):\n",
    "    meters = MultiAverageMeter()\n",
    "    # Model on train mode\n",
    "    model.train()\n",
    "    global iteration\n",
    "           \n",
    "    intersection = 0\n",
    "    union = 0\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    end = time.time()\n",
    "    for batch_idx, (x, y) in enumerate(loader): #tira uma amostra indexada no formato [idx, (x_train,y_train)]\n",
    "        # Create vaiables\n",
    "        \n",
    "        x = to_var(x) #armazena em formato pra GPU (cuda)\n",
    "        y = to_var(y)\n",
    "        # compute output\n",
    "        pred_logit = model(x) \n",
    "        loss = soft_dice_loss(pred_logit, y, smooth=1e-2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # measure accuracy and record loss\n",
    "        y = y.long()\n",
    "        batch_size = y.size(0)\n",
    "        iou = cal_batch_iou(pred_logit, y)\n",
    "        dice = cal_batch_dice(pred_logit, y)\n",
    "\n",
    "        logs = [loss.item(), iou[1:].mean(), dice[1:].mean()]+ \\\n",
    "                            [iou[i].item() for i in range(len(iou))]+ \\\n",
    "                            [dice[i].item() for i in range(len(dice))]+ \\\n",
    "                            [time.time() - end]\n",
    "        \n",
    "        \n",
    "        \n",
    "        meters.update(logs, batch_size)   \n",
    "        writer.add_scalar('train_loss_logs', loss.item(), iteration)\n",
    "        with open(os.path.join(cfg.save, 'loss_logs.csv'), 'a') as f:\n",
    "            f.write('%09d,%0.6f,\\n'%((iteration + 1),loss.item(),))\n",
    "        iteration += 1\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        # measure elapsed time\n",
    "        end = time.time()\n",
    "        # print stats\n",
    "        print_freq = 2 // meters.val[-1] + 1\n",
    "        if batch_idx % print_freq == 0:\n",
    "            res = '\\t'.join([\n",
    "                'Epoch: [%d/%d]' % (epoch + 1, n_epochs),\n",
    "                'Iter: [%d/%d]' % (batch_idx + 1, len(loader)),\n",
    "                'Time %.3f (%.3f)' % (meters.val[-1], meters.avg[-1]),\n",
    "                'Loss %.4f (%.4f)' % (meters.val[0], meters.avg[0]),\n",
    "                'IOU %.4f (%.4f)' % (meters.val[1], meters.avg[1]),\n",
    "                'DICE %.4f (%.4f)' % (meters.val[2], meters.avg[2]),\n",
    "            ])\n",
    "  \n",
    "\n",
    "    return meters.avg[:-1]\n",
    "\n",
    "\n",
    "def test_epoch(model, loader, epoch, print_freq=1, is_test=True, writer=None):\n",
    "    meters = MultiAverageMeter()\n",
    "    # Model on eval mode\n",
    "    \n",
    "    global iteration\n",
    "        \n",
    "    intersection = 0\n",
    "    union = 0\n",
    "    model.eval()\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (x, y) in enumerate(loader):\n",
    "            # Create vaiables\n",
    "            x = to_var(x)\n",
    "            y = to_var(y)\n",
    "            # compute output\n",
    "            pred_logit = model(x)\n",
    "            loss = soft_dice_loss(pred_logit, y, smooth=1e-2)\n",
    "\n",
    "            #auroc= AUROC_per_case(pred_logit, y)\n",
    "            \n",
    "            \n",
    "            # measure accuracy and record loss\n",
    "            batch_size = y.size(0)\n",
    "            iou = cal_batch_iou(pred_logit, y)\n",
    "            dice = cal_batch_dice(pred_logit, y)\n",
    "\n",
    "            logs = [loss.item(), iou[1:].mean(), dice[1:].mean()]+ \\\n",
    "                                [iou[i].item() for i in range(len(iou))]+ \\\n",
    "                                [dice[i].item() for i in range(len(dice))]+ \\\n",
    "                                [time.time() - end]\n",
    "            meters.update(logs, batch_size)\n",
    "\n",
    "            # measure elapsed time\n",
    "            end = time.time()\n",
    "\n",
    "            # print stats\n",
    "            print_freq = 2 // meters.val[-1] + 1\n",
    "            if batch_idx % print_freq == 0:\n",
    "                res = '\\t'.join([\n",
    "                    'Test' if is_test else 'Valid',\n",
    "                    'Iter: [%d/%d]' % (batch_idx + 1, len(loader)),\n",
    "                    'Time %.3f (%.3f)' % (meters.val[-1], meters.avg[-1]),\n",
    "                    'Loss %.4f (%.4f)' % (meters.val[0], meters.avg[0]),\n",
    "                    'IOU %.4f (%.4f)' % (meters.val[1], meters.avg[1]),\n",
    "                    'DICE %.4f (%.4f)' % (meters.val[2], meters.avg[2]),\n",
    "                ])\n",
    "                print(res)\n",
    "    return meters.avg[:-1]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    fire.Fire(main)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "x= pd.read_csv(r'C:\\Users\\admin\\Desktop\\cnn-facies-classifier-master\\tmp\\shape_880samples\\logs.csv')\n",
    "\n",
    "x[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Test\tIter: [1/14]\tTime 0.081 (0.081)\tLoss 0.5070 (0.5070)\tIOU 0.3060 (0.3060)\tDICE 0.4427 (0.4427)\n",
    "Current best dice: 0.3912\n",
    "Test\tIter: [1/14]\tTime 0.080 (0.080)\tLoss 0.5068 (0.5068)\tIOU 0.3076 (0.3076)\tDICE 0.4444 (0.4444)\n",
    "New best dice: 0.3915\n",
    "Test\tIter: [1/14]\tTime 0.087 (0.087)\tLoss 0.5064 (0.5064)\tIOU 0.3073 (0.3073)\tDICE 0.4439 (0.4439)\n",
    "Current best dice: 0.3915\n",
    "Test\tIter: [1/14]\tTime 0.093 (0.093)\tLoss 0.5063 (0.5063)\tIOU 0.3077 (0.3077)\tDICE 0.4443 (0.4443)\n",
    "New best dice: 0.3916\n",
    "Test\tIter: [1/14]\tTime 0.072 (0.072)\tLoss 0.5058 (0.5058)\tIOU 0.3080 (0.3080)\tDICE 0.4446 (0.4446)\n",
    "New best dice: 0.3918\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "start_time = timeit.default_timer()\n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "    print(elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yone= keras.utils.to_categorical(data['segs'], num_classes)\n",
    "yone=yone[0,:,:,:]\n",
    "yone=np.moveaxis(yone,-1,0)\n",
    "yone.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display_html\n",
    "def restartkernel() :\n",
    "    display_html(\"<script>Jupyter.notebook.kernel.restart()</script>\",raw=True)\n",
    "\n",
    "restartkernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding: utf-8\n",
    "#POC 2,5D unet pre trained\n",
    "\n",
    "\n",
    "\n",
    "import _init_paths\n",
    "\n",
    "import fire\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import sklearn\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from poc_dataset_ACS import BaseDatasetVoxel\n",
    "from mylib.loss import soft_cross_entropy_loss\n",
    "from mylib.utils import MultiAverageMeter, save_model, log_results, to_var, set_seed, \\\n",
    "        to_device, initialize, categorical_to_one_hot, copy_file_backup, redirect_stdout\n",
    "from poc_config_2dpre import POCVoxelConfig as cfg\n",
    "from poc_config_2dpre import POCVoxelEnv as env\n",
    "\n",
    "from unet import UNet\n",
    "from acsconv.models import ACSUNet\n",
    "from acsconv.converters import ACSConverter, Conv3dConverter, Conv2_5dConverter\n",
    "\n",
    "from mylib.metrics import cal_batch_iou, cal_batch_dice, AUROC_per_case\n",
    "from mylib.loss import soft_dice_loss\n",
    "\n",
    "import timeit\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "    \n",
    "def main(save_path=cfg.save, \n",
    "         n_epochs=cfg.n_epochs, \n",
    "         seed=cfg.seed\n",
    "         ):\n",
    "    if seed is not None:\n",
    "        set_seed(cfg.seed)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "\n",
    "\n",
    "    # # Models\n",
    "    os.makedirs(save_path,exist_ok = True)\n",
    "    #copy_file_backup(save_path)\n",
    "    redirect_stdout(save_path)\n",
    "\n",
    "    # Datasets\n",
    "    train_data = env.data_train\n",
    "    test_data = env.data_train\n",
    "    shape_cp = env.shape_checkpoint\n",
    "    #val_data=env.data_test\n",
    "\n",
    "\n",
    "\n",
    "    train_set = BaseDatasetVoxel(train_data, cfg.train_samples)\n",
    "    valid_set = None\n",
    "    test_set = BaseDatasetVoxel(test_data, cfg.train_samples)\n",
    "\n",
    "    model = UNet(4)\n",
    "    if cfg.conv == 'Conv3D':\n",
    "        model = Conv3dConverter(model)\n",
    "        initialize(model.modules())\n",
    "    elif cfg.conv == 'Conv2_5D':\n",
    "        print('REDE 2,5D')\n",
    "        if cfg.pretrained:\n",
    "            shape_cp = torch.load(shape_cp)\n",
    "            shape_cp.popitem()\n",
    "            shape_cp.popitem()\n",
    "            incompatible_keys = model.load_state_dict(shape_cp, strict=False)\n",
    "            print('load shape pretrained weights\\n', incompatible_keys)\n",
    "        model = Conv2_5dConverter(model)\n",
    "    elif cfg.conv == 'ACSConv':\n",
    "        # You can use either the naive ``ACSUNet`` or the ``ACSConverter(model)``\n",
    "        #model = ACSConverter(model)\n",
    "        model = ACSUNet(4)\n",
    "        if cfg.pretrained:\n",
    "            shape_cp = torch.load(shape_cp)\n",
    "            shape_cp.popitem()\n",
    "            shape_cp.popitem()\n",
    "            incompatible_keys = model.load_state_dict(shape_cp, strict=False)\n",
    "            print('load shape pretrained weights\\n', incompatible_keys)\n",
    "    else:\n",
    "        raise ValueError('not valid conv')\n",
    "    \n",
    "    print(model)\n",
    "    torch.save(model.state_dict(), os.path.join(save_path, 'model.dat'))\n",
    "    # Train the model\n",
    "    train(model=model, train_set=train_set, valid_set=valid_set, test_set=test_set, save=save_path, n_epochs=n_epochs)\n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "    print(elapsed)\n",
    "    print('Done!')\n",
    "\n",
    "\n",
    "\n",
    "def train(model, train_set, test_set, save, valid_set, n_epochs):\n",
    "\n",
    "\n",
    "    # Data loaders\n",
    "    train_loader = DataLoader(train_set, batch_size=cfg.train_batch_size, shuffle=True,\n",
    "                                pin_memory=(torch.cuda.is_available()), num_workers=cfg.num_workers)\n",
    "    test_loader = DataLoader(test_set, batch_size=cfg.test_batch_size, shuffle=False,\n",
    "                                pin_memory=(torch.cuda.is_available()), num_workers=cfg.num_workers)\n",
    "    if valid_set is None:\n",
    "        valid_loader = None\n",
    "    else:\n",
    "        valid_loader = DataLoader(valid_set, batch_size=cfg.batch_size, shuffle=False,\n",
    "                                pin_memory=(torch.cuda.is_available()), num_workers=cfg.num_workers)\n",
    "    # Model on cuda\n",
    "    model = to_device(model)\n",
    "\n",
    "    # Wrap model for multi-GPUs, if necessary\n",
    "    model_wrapper = model\n",
    "    if torch.cuda.is_available() and torch.cuda.device_count() > 1:\n",
    "        model_wrapper = torch.nn.DataParallel(model).cuda()\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.Adam(model_wrapper.parameters(), lr=cfg.lr)\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=cfg.milestones,\n",
    "                                                     gamma=cfg.gamma)\n",
    "\n",
    "    # Start log\n",
    "    logs = ['loss', 'iou', 'dice'] + ['iou{}'.format(i) for i in range(4)]+['dice{}'.format(i) for i in range(4)]\n",
    "    train_logs = ['train_'+log for log in logs]\n",
    "    test_logs = ['test_'+log for log in logs]\n",
    "    log_dict = OrderedDict.fromkeys(train_logs+test_logs, 0)\n",
    "    with open(os.path.join(save, 'logs.csv'), 'w') as f:\n",
    "        f.write('epoch,')\n",
    "        for key in log_dict.keys():\n",
    "            f.write(key+',')\n",
    "        f.write('\\n')\n",
    "    writer = SummaryWriter(log_dir=os.path.join(save, 'Tensorboard_Results'))\n",
    "\n",
    "    # Train model\n",
    "    best_dice = 0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        os.makedirs(os.path.join(cfg.save, 'epoch_{}'.format(epoch)),exist_ok = True)\n",
    "        train_meters = train_epoch(\n",
    "            model=model_wrapper,\n",
    "            loader=train_loader,\n",
    "            optimizer=optimizer,\n",
    "            epoch=epoch,\n",
    "            n_epochs=n_epochs,\n",
    "            writer=writer\n",
    "        )\n",
    "        # if (epoch+1)%5==0:\n",
    "        test_meters = test_epoch(\n",
    "            model=model_wrapper,\n",
    "            loader=test_loader,\n",
    "            epoch=epoch,\n",
    "            is_test=True,\n",
    "            writer = writer\n",
    "        )\n",
    "        scheduler.step()\n",
    "\n",
    "        # Log results\n",
    "        for i, key in enumerate(train_logs):\n",
    "            log_dict[key] = train_meters[i]\n",
    "        for i, key in enumerate(test_logs):\n",
    "            log_dict[key] = test_meters[i]\n",
    "\n",
    "        log_results(save, epoch, log_dict, writer=writer)\n",
    "\n",
    "        if cfg.save_all:\n",
    "            torch.save(model.state_dict(), os.path.join(save, 'epoch_{}'.format(epoch), 'model.dat'))\n",
    "\n",
    "        if log_dict['test_dice'] > best_dice:\n",
    "            torch.save(model.state_dict(), os.path.join(save, 'model.dat'))\n",
    "            best_dice = log_dict['test_dice']\n",
    "            print('New best dice: %.4f' % log_dict['test_dice'])\n",
    "            #print(2.*intersection/union)\n",
    "        else:\n",
    "            print('Current best dice: %.4f' % best_dice)\n",
    "            #print(2.*intersection/union)\n",
    "    writer.close()\n",
    "\n",
    "    with open(os.path.join(save, 'logs.csv'), 'a') as f:\n",
    "        f.write(',,,,best dice,%0.5f\\n' % (best_dice))\n",
    "    # Final test of the best model on test set\n",
    "    print('best dice: ', best_dice)\n",
    "\n",
    "iteration = 0\n",
    "\n",
    "def train_epoch(model, loader, optimizer, epoch, n_epochs, print_freq=1, writer=None):\n",
    "    meters = MultiAverageMeter()\n",
    "    # Model on train mode\n",
    "    model.train()\n",
    "    global iteration\n",
    "    end = time.time()\n",
    "    for batch_idx, (x, y) in enumerate(loader):\n",
    "        # Create vaiables\n",
    "        x = to_var(x)\n",
    "        y = to_var(y)\n",
    "        # compute output\n",
    "        pred_logit = model(x)\n",
    "        loss = soft_dice_loss(pred_logit, y, smooth=1e-2)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        y = y.long()\n",
    "        \n",
    "        batch_size = y.size(0)\n",
    "        iou = cal_batch_iou(pred_logit, y)\n",
    "        dice = cal_batch_dice(pred_logit, y)\n",
    "\n",
    "        logs = [loss.item(), iou[1:].mean(), dice[1:].mean()]+ \\\n",
    "                            [iou[i].item() for i in range(len(iou))]+ \\\n",
    "                            [dice[i].item() for i in range(len(dice))]+ \\\n",
    "                            [time.time() - end]\n",
    "        meters.update(logs, batch_size)   \n",
    "        writer.add_scalar('train_loss_logs', loss.item(), iteration)\n",
    "        with open(os.path.join(cfg.save, 'loss_logs.csv'), 'a') as f:\n",
    "            f.write('%09d,%0.6f,\\n'%((iteration + 1),loss.item(),))\n",
    "        iteration += 1\n",
    "\n",
    "\n",
    "        # measure elapsed time\n",
    "        end = time.time()\n",
    "        # print stats\n",
    "        print_freq = 2 // meters.val[-1] + 1\n",
    "        if batch_idx % print_freq == 0:\n",
    "            res = '\\t'.join([\n",
    "                'Epoch: [%d/%d]' % (epoch + 1, n_epochs),\n",
    "                'Iter: [%d/%d]' % (batch_idx + 1, len(loader)),\n",
    "                'Time %.3f (%.3f)' % (meters.val[-1], meters.avg[-1]),\n",
    "                'Loss %.4f (%.4f)' % (meters.val[0], meters.avg[0]),\n",
    "                'IOU %.4f (%.4f)' % (meters.val[1], meters.avg[1]),\n",
    "                'DICE %.4f (%.4f)' % (meters.val[2], meters.avg[2]),\n",
    "            ])\n",
    "            print(res)\n",
    "\n",
    "    return meters.avg[:-1] #intersection, union\n",
    "\n",
    "\n",
    "def test_epoch(model, loader, epoch, print_freq=1, is_test=True, writer=None):\n",
    "    meters = MultiAverageMeter()\n",
    "    # Model on eval mode\n",
    "    model.eval()\n",
    "    gt_classes = []\n",
    "    pred_all_probs = []\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (x, y) in enumerate(loader):\n",
    "            \n",
    "            x = to_var(x)\n",
    "            \n",
    "            \n",
    "            y = to_var(y)\n",
    "            \n",
    "            pred_logit = model(x)\n",
    "            \n",
    "            # calculate metrics\n",
    "            pred_class = pred_logit.max(dim=1)[1]\n",
    "            pred_probs = pred_logit.softmax(-1)\n",
    "            pred_all_probs.append(pred_probs.cpu())\n",
    "            gt_classes.append(y.cpu())\n",
    "            \n",
    "            #print(gt_classes.shape) #pred_class[20,48,48,48]\n",
    "            #print(pred_probs[1]) #y e pred_probs[20,6,48,48,48]\n",
    "            \n",
    "            batch_size, n_classes = pred_logit.shape[:2]\n",
    "            \n",
    "\n",
    "            loss = soft_dice_loss(pred_logit, y, smooth=1e-2)\n",
    "            y = y.long()\n",
    "            batch_size = y.size(0)\n",
    "            iou = cal_batch_iou(pred_logit, y)\n",
    "            dice = cal_batch_dice(pred_logit, y)\n",
    " \n",
    "            logs = [loss.item(), iou[1:].mean(), dice[1:].mean()]+ \\\n",
    "                                [iou[i].item() for i in range(len(iou))]+ \\\n",
    "                                [dice[i].item() for i in range(len(dice))]+ \\\n",
    "                                [time.time() - end]\n",
    "            meters.update(logs, batch_size)\n",
    "\n",
    "            end = time.time()\n",
    "\n",
    "            print_freq = 2 // meters.val[-1] + 1\n",
    "            if batch_idx % print_freq == 0:\n",
    "                res = '\\t'.join([\n",
    "                    'Test' if is_test else 'Valid',\n",
    "                    'Iter: [%d/%d]' % (batch_idx + 1, len(loader)),\n",
    "                    'Time %.3f (%.3f)' % (meters.val[-1], meters.avg[-1]),\n",
    "                    'Loss %.4f (%.4f)' % (meters.val[0], meters.avg[0]),\n",
    "                    'IOU %.4f (%.4f)' % (meters.val[1], meters.avg[1]),\n",
    "                    'DICE %.4f (%.4f)' % (meters.val[2], meters.avg[2]),\n",
    "                ])\n",
    "                print(res)\n",
    "\n",
    "    return meters.avg[:-1]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    fire.Fire(main)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding: utf-8\n",
    "#POC ACS conv UNET pre-trained\n",
    "import _init_paths\n",
    "import timeit\n",
    "start_time = timeit.default_timer()\n",
    "# code you want to evaluate\n",
    "\n",
    "import fire\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import sklearn\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from poc_dataset_ACS import BaseDatasetVoxel\n",
    "from mylib.loss import soft_cross_entropy_loss\n",
    "from mylib.utils import MultiAverageMeter, save_model, log_results, to_var, set_seed, \\\n",
    "        to_device, initialize, categorical_to_one_hot, copy_file_backup, redirect_stdout\n",
    "from poc_config import POCVoxelConfig as cfg\n",
    "from poc_config import POCVoxelEnv as env\n",
    "\n",
    "from unet import UNet\n",
    "from acsconv.models import ACSUNet\n",
    "from acsconv.converters import ACSConverter, Conv3dConverter, Conv2_5dConverter\n",
    "\n",
    "from mylib.metrics import cal_batch_iou, cal_batch_dice\n",
    "from mylib.loss import soft_dice_loss\n",
    "import timeit\n",
    "start_time = timeit.default_timer()\n",
    "# code you want to evaluate\n",
    "\n",
    "def main(save_path=cfg.save, \n",
    "         n_epochs=cfg.n_epochs, \n",
    "         seed=cfg.seed\n",
    "         ):\n",
    "    if seed is not None:\n",
    "        set_seed(cfg.seed)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "\n",
    "\n",
    "    # # Models\n",
    "    os.makedirs(save_path,exist_ok = True)\n",
    "    #copy_file_backup(save_path)\n",
    "    redirect_stdout(save_path)\n",
    "\n",
    "    # Datasets\n",
    "    train_data = env.data_train\n",
    "    test_data = env.data_train\n",
    "    shape_cp = env.shape_checkpoint\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    train_set = BaseDatasetVoxel(train_data, cfg.train_samples)\n",
    "    valid_set = None\n",
    "    test_set = BaseDatasetVoxel(test_data, cfg.train_samples)\n",
    "\n",
    "    model = UNet(4)\n",
    "    if cfg.conv == 'Conv3D':\n",
    "        model = Conv3dConverter(model)\n",
    "        initialize(model.modules())\n",
    "    elif cfg.conv == 'Conv2_5D':\n",
    "        print('REDE 2,5D')\n",
    "        if cfg.pretrained:\n",
    "            shape_cp = torch.load(shape_cp)\n",
    "            shape_cp.popitem()\n",
    "            shape_cp.popitem()\n",
    "            incompatible_keys = model.load_state_dict(shape_cp, strict=False)\n",
    "            print('load shape pretrained weights\\n', incompatible_keys)\n",
    "        model = Conv2_5dConverter(model)\n",
    "    elif cfg.conv == 'ACSConv':\n",
    "        # You can use either the naive ``ACSUNet`` or the ``ACSConverter(model)``\n",
    "        #model = ACSConverter(model)\n",
    "        model = ACSUNet(4)\n",
    "        if cfg.pretrained:\n",
    "            shape_cp = torch.load(shape_cp)\n",
    "            shape_cp.popitem()\n",
    "            shape_cp.popitem()\n",
    "            incompatible_keys = model.load_state_dict(shape_cp, strict=False)\n",
    "            print('load shape pretrained weights\\n', incompatible_keys)\n",
    "    else:\n",
    "        raise ValueError('not valid conv')\n",
    "    \n",
    "    print(model)\n",
    "    torch.save(model.state_dict(), os.path.join(save_path, 'model.dat'))\n",
    "    # Train the model\n",
    "    train(model=model, train_set=train_set, valid_set=valid_set, test_set=test_set, save=save_path, n_epochs=n_epochs)\n",
    "    \n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "    print(elapsed)\n",
    "    print('Done!')\n",
    "\n",
    "\n",
    "\n",
    "def train(model, train_set, test_set, save, valid_set, n_epochs):\n",
    "\n",
    "\n",
    "    # Data loaders\n",
    "    train_loader = DataLoader(train_set, batch_size=cfg.train_batch_size, shuffle=True,\n",
    "                                pin_memory=(torch.cuda.is_available()), num_workers=cfg.num_workers)\n",
    "    test_loader = DataLoader(test_set, batch_size=cfg.test_batch_size, shuffle=False,\n",
    "                                pin_memory=(torch.cuda.is_available()), num_workers=cfg.num_workers)\n",
    "    if valid_set is None:\n",
    "        valid_loader = None\n",
    "    else:\n",
    "        valid_loader = DataLoader(valid_set, batch_size=cfg.batch_size, shuffle=False,\n",
    "                                pin_memory=(torch.cuda.is_available()), num_workers=cfg.num_workers)\n",
    "    # Model on cuda\n",
    "    model = to_device(model)\n",
    "\n",
    "    # Wrap model for multi-GPUs, if necessary\n",
    "    model_wrapper = model\n",
    "    if torch.cuda.is_available() and torch.cuda.device_count() > 1:\n",
    "        model_wrapper = torch.nn.DataParallel(model).cuda()\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.Adam(model_wrapper.parameters(), lr=cfg.lr)\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=cfg.milestones,\n",
    "                                                     gamma=cfg.gamma)\n",
    "\n",
    "    # Start log\n",
    "    logs = ['loss', 'iou', 'dice'] + ['iou{}'.format(i) for i in range(4)]+['dice{}'.format(i) for i in range(4)]\n",
    "    train_logs = ['train_'+log for log in logs]\n",
    "    test_logs = ['test_'+log for log in logs]\n",
    "    log_dict = OrderedDict.fromkeys(train_logs+test_logs, 0)\n",
    "    with open(os.path.join(save, 'logs.csv'), 'w') as f:\n",
    "        f.write('epoch,')\n",
    "        for key in log_dict.keys():\n",
    "            f.write(key+',')\n",
    "        f.write('\\n')\n",
    "    writer = SummaryWriter(log_dir=os.path.join(save, 'Tensorboard_Results'))\n",
    "\n",
    "    # Train model\n",
    "    best_dice = 0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        os.makedirs(os.path.join(cfg.save, 'epoch_{}'.format(epoch)),exist_ok = True)\n",
    "        train_meters = train_epoch(\n",
    "            model=model_wrapper,\n",
    "            loader=train_loader,\n",
    "            optimizer=optimizer,\n",
    "            epoch=epoch,\n",
    "            n_epochs=n_epochs,\n",
    "            writer=writer\n",
    "        )\n",
    "        # if (epoch+1)%5==0:\n",
    "        test_meters = test_epoch(\n",
    "            model=model_wrapper,\n",
    "            loader=test_loader,\n",
    "            epoch=epoch,\n",
    "            is_test=True,\n",
    "            writer = writer\n",
    "        )\n",
    "        scheduler.step()\n",
    "\n",
    "        # Log results\n",
    "        for i, key in enumerate(train_logs):\n",
    "            log_dict[key] = train_meters[i]\n",
    "        for i, key in enumerate(test_logs):\n",
    "            log_dict[key] = test_meters[i]\n",
    "\n",
    "        log_results(save, epoch, log_dict, writer=writer)\n",
    "\n",
    "        if cfg.save_all:\n",
    "            torch.save(model.state_dict(), os.path.join(save, 'epoch_{}'.format(epoch), 'model.dat'))\n",
    "\n",
    "        if log_dict['test_dice'] > best_dice:\n",
    "            torch.save(model.state_dict(), os.path.join(save, 'model.dat'))\n",
    "            best_dice = log_dict['test_dice']\n",
    "            print('New best dice: %.4f' % log_dict['test_dice'])\n",
    "            #print(2.*intersection/union)\n",
    "        else:\n",
    "            print('Current best dice: %.4f' % best_dice)\n",
    "            #print(2.*intersection/union)\n",
    "    writer.close()\n",
    "\n",
    "    with open(os.path.join(save, 'logs.csv'), 'a') as f:\n",
    "        f.write(',,,,best dice,%0.5f\\n' % (best_dice))\n",
    "    # Final test of the best model on test set\n",
    "    print('best dice: ', best_dice)\n",
    "\n",
    "iteration = 0\n",
    "\n",
    "def train_epoch(model, loader, optimizer, epoch, n_epochs, print_freq=1, writer=None):\n",
    "    meters = MultiAverageMeter()\n",
    "    # Model on train mode\n",
    "    model.train()\n",
    "    global iteration\n",
    "    end = time.time()\n",
    "    for batch_idx, (x, y) in enumerate(loader):\n",
    "        # Create vaiables\n",
    "        x = to_var(x)\n",
    "        y = to_var(y)\n",
    "        # compute output\n",
    "        pred_logit = model(x)\n",
    "        loss = soft_dice_loss(pred_logit, y, smooth=1e-2)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        y = y.long()\n",
    "        \n",
    "        batch_size = y.size(0)\n",
    "        iou = cal_batch_iou(pred_logit, y)\n",
    "        dice = cal_batch_dice(pred_logit, y)\n",
    "\n",
    "        logs = [loss.item(), iou[1:].mean(), dice[1:].mean()]+ \\\n",
    "                            [iou[i].item() for i in range(len(iou))]+ \\\n",
    "                            [dice[i].item() for i in range(len(dice))]+ \\\n",
    "                            [time.time() - end]\n",
    "        meters.update(logs, batch_size)   \n",
    "        writer.add_scalar('train_loss_logs', loss.item(), iteration)\n",
    "        with open(os.path.join(cfg.save, 'loss_logs.csv'), 'a') as f:\n",
    "            f.write('%09d,%0.6f,\\n'%((iteration + 1),loss.item(),))\n",
    "        iteration += 1\n",
    "\n",
    "\n",
    "        # measure elapsed time\n",
    "        end = time.time()\n",
    "        # print stats\n",
    "        print_freq = 2 // meters.val[-1] + 1\n",
    "        if batch_idx % print_freq == 0:\n",
    "            res = '\\t'.join([\n",
    "                'Epoch: [%d/%d]' % (epoch + 1, n_epochs),\n",
    "                'Iter: [%d/%d]' % (batch_idx + 1, len(loader)),\n",
    "                'Time %.3f (%.3f)' % (meters.val[-1], meters.avg[-1]),\n",
    "                'Loss %.4f (%.4f)' % (meters.val[0], meters.avg[0]),\n",
    "                'IOU %.4f (%.4f)' % (meters.val[1], meters.avg[1]),\n",
    "                'DICE %.4f (%.4f)' % (meters.val[2], meters.avg[2]),\n",
    "            ])\n",
    "            print(res)\n",
    "\n",
    "    return meters.avg[:-1] #intersection, union\n",
    "\n",
    "\n",
    "def test_epoch(model, loader, epoch, print_freq=1, is_test=True, writer=None):\n",
    "    meters = MultiAverageMeter()\n",
    "    # Model on eval mode\n",
    "    model.eval()\n",
    "    gt_classes = []\n",
    "    pred_all_probs = []\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (x, y) in enumerate(loader):\n",
    "            \n",
    "            x = to_var(x)\n",
    "            \n",
    "            \n",
    "            y = to_var(y)\n",
    "            \n",
    "            pred_logit = model(x)\n",
    "            \n",
    "            # calculate metrics\n",
    "            pred_class = pred_logit.max(dim=1)[1]\n",
    "            pred_probs = pred_logit.softmax(-1)\n",
    "            pred_all_probs.append(pred_probs.cpu())\n",
    "            gt_classes.append(y.cpu())\n",
    "            \n",
    "            #print(gt_classes.shape) #pred_class[20,48,48,48]\n",
    "            #print(pred_probs[1]) #y e pred_probs[20,6,48,48,48]\n",
    "            \n",
    "            batch_size, n_classes = pred_logit.shape[:2]\n",
    "            \n",
    "\n",
    "            loss = soft_dice_loss(pred_logit, y, smooth=1e-2)\n",
    "            y = y.long()\n",
    "            batch_size = y.size(0)\n",
    "            iou = cal_batch_iou(pred_logit, y)\n",
    "            dice = cal_batch_dice(pred_logit, y)\n",
    " \n",
    "            logs = [loss.item(), iou[1:].mean(), dice[1:].mean()]+ \\\n",
    "                                [iou[i].item() for i in range(len(iou))]+ \\\n",
    "                                [dice[i].item() for i in range(len(dice))]+ \\\n",
    "                                [time.time() - end]\n",
    "            meters.update(logs, batch_size)\n",
    "\n",
    "            end = time.time()\n",
    "\n",
    "            print_freq = 2 // meters.val[-1] + 1\n",
    "            if batch_idx % print_freq == 0:\n",
    "                res = '\\t'.join([\n",
    "                    'Test' if is_test else 'Valid',\n",
    "                    'Iter: [%d/%d]' % (batch_idx + 1, len(loader)),\n",
    "                    'Time %.3f (%.3f)' % (meters.val[-1], meters.avg[-1]),\n",
    "                    'Loss %.4f (%.4f)' % (meters.val[0], meters.avg[0]),\n",
    "                    'IOU %.4f (%.4f)' % (meters.val[1], meters.avg[1]),\n",
    "                    'DICE %.4f (%.4f)' % (meters.val[2], meters.avg[2]),\n",
    "                ])\n",
    "                print(res)\n",
    "\n",
    "    return meters.avg[:-1]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    fire.Fire(main)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding: utf-8\n",
    "#POC ACS conv UNET random\n",
    "import _init_paths\n",
    "import timeit\n",
    "start_time = timeit.default_timer()\n",
    "# code you want to evaluate\n",
    "\n",
    "import fire\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import sklearn\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from poc_dataset_ACS import BaseDatasetVoxel\n",
    "from mylib.loss import soft_cross_entropy_loss\n",
    "from mylib.utils import MultiAverageMeter, save_model, log_results, to_var, set_seed, \\\n",
    "        to_device, initialize, categorical_to_one_hot, copy_file_backup, redirect_stdout\n",
    "from poc_config_ACSUNETr import POCVoxelConfig as cfg\n",
    "from poc_config_ACSUNETr import POCVoxelEnv as env\n",
    "\n",
    "from unet import UNet\n",
    "from acsconv.models import ACSUNet\n",
    "from acsconv.converters import ACSConverter, Conv3dConverter, Conv2_5dConverter\n",
    "\n",
    "from mylib.metrics import cal_batch_iou, cal_batch_dice\n",
    "from mylib.loss import soft_dice_loss\n",
    "import timeit\n",
    "start_time = timeit.default_timer()\n",
    "# code you want to evaluate\n",
    "\n",
    "def main(save_path=cfg.save, \n",
    "         n_epochs=cfg.n_epochs, \n",
    "         seed=cfg.seed\n",
    "         ):\n",
    "    if seed is not None:\n",
    "        set_seed(cfg.seed)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "\n",
    "\n",
    "    # # Models\n",
    "    os.makedirs(save_path,exist_ok = True)\n",
    "    #copy_file_backup(save_path)\n",
    "    redirect_stdout(save_path)\n",
    "\n",
    "    # Datasets\n",
    "    train_data = env.data_train\n",
    "    test_data = env.data_train\n",
    "    shape_cp = env.shape_checkpoint\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    train_set = BaseDatasetVoxel(train_data, cfg.train_samples)\n",
    "    valid_set = None\n",
    "    test_set = BaseDatasetVoxel(test_data, cfg.train_samples)\n",
    "\n",
    "    model = UNet(4)\n",
    "    if cfg.conv == 'Conv3D':\n",
    "        model = Conv3dConverter(model)\n",
    "        initialize(model.modules())\n",
    "    elif cfg.conv == 'Conv2_5D':\n",
    "        print('REDE 2,5D')\n",
    "        if cfg.pretrained:\n",
    "            shape_cp = torch.load(shape_cp)\n",
    "            shape_cp.popitem()\n",
    "            shape_cp.popitem()\n",
    "            incompatible_keys = model.load_state_dict(shape_cp, strict=False)\n",
    "            print('load shape pretrained weights\\n', incompatible_keys)\n",
    "        model = Conv2_5dConverter(model)\n",
    "    elif cfg.conv == 'ACSConv':\n",
    "        # You can use either the naive ``ACSUNet`` or the ``ACSConverter(model)``\n",
    "        #model = ACSConverter(model)\n",
    "        model = ACSUNet(4)\n",
    "        if cfg.pretrained:\n",
    "            shape_cp = torch.load(shape_cp)\n",
    "            shape_cp.popitem()\n",
    "            shape_cp.popitem()\n",
    "            incompatible_keys = model.load_state_dict(shape_cp, strict=False)\n",
    "            print('load shape pretrained weights\\n', incompatible_keys)\n",
    "    else:\n",
    "        raise ValueError('not valid conv')\n",
    "    \n",
    "    print(model)\n",
    "    torch.save(model.state_dict(), os.path.join(save_path, 'model.dat'))\n",
    "    # Train the model\n",
    "    train(model=model, train_set=train_set, valid_set=valid_set, test_set=test_set, save=save_path, n_epochs=n_epochs)\n",
    "    \n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "    print(elapsed)\n",
    "    print('Done!')\n",
    "\n",
    "\n",
    "\n",
    "def train(model, train_set, test_set, save, valid_set, n_epochs):\n",
    "\n",
    "\n",
    "    # Data loaders\n",
    "    train_loader = DataLoader(train_set, batch_size=cfg.train_batch_size, shuffle=True,\n",
    "                                pin_memory=(torch.cuda.is_available()), num_workers=cfg.num_workers)\n",
    "    test_loader = DataLoader(test_set, batch_size=cfg.test_batch_size, shuffle=False,\n",
    "                                pin_memory=(torch.cuda.is_available()), num_workers=cfg.num_workers)\n",
    "    if valid_set is None:\n",
    "        valid_loader = None\n",
    "    else:\n",
    "        valid_loader = DataLoader(valid_set, batch_size=cfg.batch_size, shuffle=False,\n",
    "                                pin_memory=(torch.cuda.is_available()), num_workers=cfg.num_workers)\n",
    "    # Model on cuda\n",
    "    model = to_device(model)\n",
    "\n",
    "    # Wrap model for multi-GPUs, if necessary\n",
    "    model_wrapper = model\n",
    "    if torch.cuda.is_available() and torch.cuda.device_count() > 1:\n",
    "        model_wrapper = torch.nn.DataParallel(model).cuda()\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.Adam(model_wrapper.parameters(), lr=cfg.lr)\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=cfg.milestones,\n",
    "                                                     gamma=cfg.gamma)\n",
    "\n",
    "    # Start log\n",
    "    logs = ['loss', 'iou', 'dice'] + ['iou{}'.format(i) for i in range(4)]+['dice{}'.format(i) for i in range(4)]\n",
    "    train_logs = ['train_'+log for log in logs]\n",
    "    test_logs = ['test_'+log for log in logs]\n",
    "    log_dict = OrderedDict.fromkeys(train_logs+test_logs, 0)\n",
    "    with open(os.path.join(save, 'logs.csv'), 'w') as f:\n",
    "        f.write('epoch,')\n",
    "        for key in log_dict.keys():\n",
    "            f.write(key+',')\n",
    "        f.write('\\n')\n",
    "    writer = SummaryWriter(log_dir=os.path.join(save, 'Tensorboard_Results'))\n",
    "\n",
    "    # Train model\n",
    "    best_dice = 0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        os.makedirs(os.path.join(cfg.save, 'epoch_{}'.format(epoch)),exist_ok = True)\n",
    "        train_meters = train_epoch(\n",
    "            model=model_wrapper,\n",
    "            loader=train_loader,\n",
    "            optimizer=optimizer,\n",
    "            epoch=epoch,\n",
    "            n_epochs=n_epochs,\n",
    "            writer=writer\n",
    "        )\n",
    "        # if (epoch+1)%5==0:\n",
    "        test_meters = test_epoch(\n",
    "            model=model_wrapper,\n",
    "            loader=test_loader,\n",
    "            epoch=epoch,\n",
    "            is_test=True,\n",
    "            writer = writer\n",
    "        )\n",
    "        scheduler.step()\n",
    "\n",
    "        # Log results\n",
    "        for i, key in enumerate(train_logs):\n",
    "            log_dict[key] = train_meters[i]\n",
    "        for i, key in enumerate(test_logs):\n",
    "            log_dict[key] = test_meters[i]\n",
    "\n",
    "        log_results(save, epoch, log_dict, writer=writer)\n",
    "\n",
    "        if cfg.save_all:\n",
    "            torch.save(model.state_dict(), os.path.join(save, 'epoch_{}'.format(epoch), 'model.dat'))\n",
    "\n",
    "        if log_dict['test_dice'] > best_dice:\n",
    "            torch.save(model.state_dict(), os.path.join(save, 'model.dat'))\n",
    "            best_dice = log_dict['test_dice']\n",
    "            print('New best dice: %.4f' % log_dict['test_dice'])\n",
    "            #print(2.*intersection/union)\n",
    "        else:\n",
    "            print('Current best dice: %.4f' % best_dice)\n",
    "            #print(2.*intersection/union)\n",
    "    writer.close()\n",
    "\n",
    "    with open(os.path.join(save, 'logs.csv'), 'a') as f:\n",
    "        f.write(',,,,best dice,%0.5f\\n' % (best_dice))\n",
    "    # Final test of the best model on test set\n",
    "    print('best dice: ', best_dice)\n",
    "\n",
    "iteration = 0\n",
    "\n",
    "def train_epoch(model, loader, optimizer, epoch, n_epochs, print_freq=1, writer=None):\n",
    "    meters = MultiAverageMeter()\n",
    "    # Model on train mode\n",
    "    model.train()\n",
    "    global iteration\n",
    "    end = time.time()\n",
    "    for batch_idx, (x, y) in enumerate(loader):\n",
    "        # Create vaiables\n",
    "        x = to_var(x)\n",
    "        y = to_var(y)\n",
    "        # compute output\n",
    "        pred_logit = model(x)\n",
    "        loss = soft_dice_loss(pred_logit, y, smooth=1e-2)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        y = y.long()\n",
    "        \n",
    "        batch_size = y.size(0)\n",
    "        iou = cal_batch_iou(pred_logit, y)\n",
    "        dice = cal_batch_dice(pred_logit, y)\n",
    "\n",
    "        logs = [loss.item(), iou[1:].mean(), dice[1:].mean()]+ \\\n",
    "                            [iou[i].item() for i in range(len(iou))]+ \\\n",
    "                            [dice[i].item() for i in range(len(dice))]+ \\\n",
    "                            [time.time() - end]\n",
    "        meters.update(logs, batch_size)   \n",
    "        writer.add_scalar('train_loss_logs', loss.item(), iteration)\n",
    "        with open(os.path.join(cfg.save, 'loss_logs.csv'), 'a') as f:\n",
    "            f.write('%09d,%0.6f,\\n'%((iteration + 1),loss.item(),))\n",
    "        iteration += 1\n",
    "\n",
    "\n",
    "        # measure elapsed time\n",
    "        end = time.time()\n",
    "        # print stats\n",
    "        print_freq = 2 // meters.val[-1] + 1\n",
    "        if batch_idx % print_freq == 0:\n",
    "            res = '\\t'.join([\n",
    "                'Epoch: [%d/%d]' % (epoch + 1, n_epochs),\n",
    "                'Iter: [%d/%d]' % (batch_idx + 1, len(loader)),\n",
    "                'Time %.3f (%.3f)' % (meters.val[-1], meters.avg[-1]),\n",
    "                'Loss %.4f (%.4f)' % (meters.val[0], meters.avg[0]),\n",
    "                'IOU %.4f (%.4f)' % (meters.val[1], meters.avg[1]),\n",
    "                'DICE %.4f (%.4f)' % (meters.val[2], meters.avg[2]),\n",
    "            ])\n",
    "            print(res)\n",
    "\n",
    "    return meters.avg[:-1] #intersection, union\n",
    "\n",
    "\n",
    "def test_epoch(model, loader, epoch, print_freq=1, is_test=True, writer=None):\n",
    "    meters = MultiAverageMeter()\n",
    "    # Model on eval mode\n",
    "    model.eval()\n",
    "    gt_classes = []\n",
    "    pred_all_probs = []\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (x, y) in enumerate(loader):\n",
    "            \n",
    "            x = to_var(x)\n",
    "            \n",
    "            \n",
    "            y = to_var(y)\n",
    "            \n",
    "            pred_logit = model(x)\n",
    "            \n",
    "            # calculate metrics\n",
    "            pred_class = pred_logit.max(dim=1)[1]\n",
    "            pred_probs = pred_logit.softmax(-1)\n",
    "            pred_all_probs.append(pred_probs.cpu())\n",
    "            gt_classes.append(y.cpu())\n",
    "            \n",
    "            #print(gt_classes.shape) #pred_class[20,48,48,48]\n",
    "            #print(pred_probs[1]) #y e pred_probs[20,6,48,48,48]\n",
    "            \n",
    "            batch_size, n_classes = pred_logit.shape[:2]\n",
    "            \n",
    "\n",
    "            loss = soft_dice_loss(pred_logit, y, smooth=1e-2)\n",
    "            y = y.long()\n",
    "            batch_size = y.size(0)\n",
    "            iou = cal_batch_iou(pred_logit, y)\n",
    "            dice = cal_batch_dice(pred_logit, y)\n",
    " \n",
    "            logs = [loss.item(), iou[1:].mean(), dice[1:].mean()]+ \\\n",
    "                                [iou[i].item() for i in range(len(iou))]+ \\\n",
    "                                [dice[i].item() for i in range(len(dice))]+ \\\n",
    "                                [time.time() - end]\n",
    "            meters.update(logs, batch_size)\n",
    "\n",
    "            end = time.time()\n",
    "\n",
    "            print_freq = 2 // meters.val[-1] + 1\n",
    "            if batch_idx % print_freq == 0:\n",
    "                res = '\\t'.join([\n",
    "                    'Test' if is_test else 'Valid',\n",
    "                    'Iter: [%d/%d]' % (batch_idx + 1, len(loader)),\n",
    "                    'Time %.3f (%.3f)' % (meters.val[-1], meters.avg[-1]),\n",
    "                    'Loss %.4f (%.4f)' % (meters.val[0], meters.avg[0]),\n",
    "                    'IOU %.4f (%.4f)' % (meters.val[1], meters.avg[1]),\n",
    "                    'DICE %.4f (%.4f)' % (meters.val[2], meters.avg[2]),\n",
    "                ])\n",
    "                print(res)\n",
    "\n",
    "    return meters.avg[:-1]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    fire.Fire(main)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding: utf-8\n",
    "#POC unet 3D (poc_config_3D.py)\n",
    "import _init_paths\n",
    "\n",
    "import fire\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import sklearn\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from poc_dataset_ACS import BaseDatasetVoxel\n",
    "from mylib.loss import soft_cross_entropy_loss\n",
    "from mylib.utils import MultiAverageMeter, save_model, log_results, to_var, set_seed, \\\n",
    "        to_device, initialize, categorical_to_one_hot, copy_file_backup, redirect_stdout\n",
    "from poc_config_3D import POCVoxelConfig as cfg\n",
    "from poc_config_3D import POCVoxelEnv as env\n",
    "\n",
    "from unet import UNet\n",
    "from acsconv.models import ACSUNet\n",
    "from acsconv.converters import ACSConverter, Conv3dConverter, Conv2_5dConverter\n",
    "\n",
    "from mylib.metrics import cal_batch_iou, cal_batch_dice\n",
    "from mylib.loss import soft_dice_loss\n",
    "import timeit\n",
    "def main(save_path=cfg.save, \n",
    "         n_epochs=cfg.n_epochs, \n",
    "         seed=cfg.seed\n",
    "         ):\n",
    "    start_time = timeit.default_timer()\n",
    "    if seed is not None:\n",
    "        set_seed(cfg.seed)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "\n",
    "\n",
    "    # # Models\n",
    "    os.makedirs(save_path,exist_ok = True)\n",
    "    #copy_file_backup(save_path)\n",
    "    redirect_stdout(save_path)\n",
    "\n",
    "    # Datasets\n",
    "    train_data = env.data_train\n",
    "    test_data = env.data_train\n",
    "    shape_cp = env.shape_checkpoint\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    train_set = BaseDatasetVoxel(train_data, cfg.train_samples)\n",
    "    valid_set = None\n",
    "    test_set = BaseDatasetVoxel(test_data, cfg.train_samples)\n",
    "\n",
    "    model = UNet(4)\n",
    "    if cfg.conv == 'Conv3D':\n",
    "        model = Conv3dConverter(model)\n",
    "        initialize(model.modules())\n",
    "    elif cfg.conv == 'Conv2_5D':\n",
    "        print('REDE 2,5D')\n",
    "        if cfg.pretrained:\n",
    "            shape_cp = torch.load(shape_cp)\n",
    "            shape_cp.popitem()\n",
    "            shape_cp.popitem()\n",
    "            incompatible_keys = model.load_state_dict(shape_cp, strict=False)\n",
    "            print('load shape pretrained weights\\n', incompatible_keys)\n",
    "        model = Conv2_5dConverter(model)\n",
    "    elif cfg.conv == 'ACSConv':\n",
    "        # You can use either the naive ``ACSUNet`` or the ``ACSConverter(model)``\n",
    "        #model = ACSConverter(model)\n",
    "        model = ACSUNet(4)\n",
    "        if cfg.pretrained:\n",
    "            shape_cp = torch.load(shape_cp)\n",
    "            shape_cp.popitem()\n",
    "            shape_cp.popitem()\n",
    "            incompatible_keys = model.load_state_dict(shape_cp, strict=False)\n",
    "            print('load shape pretrained weights\\n', incompatible_keys)\n",
    "    else:\n",
    "        raise ValueError('not valid conv')\n",
    "    \n",
    "    print(model)\n",
    "    torch.save(model.state_dict(), os.path.join(save_path, 'model.dat'))\n",
    "    # Train the model\n",
    "    train(model=model, train_set=train_set, valid_set=valid_set, test_set=test_set, save=save_path, n_epochs=n_epochs)\n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "    print(elapsed)\n",
    "    print('Done!')\n",
    "    \n",
    "    \n",
    "\n",
    "def train(model, train_set, test_set, save, valid_set, n_epochs):\n",
    "\n",
    "\n",
    "    # Data loaders\n",
    "    train_loader = DataLoader(train_set, batch_size=cfg.train_batch_size, shuffle=True,\n",
    "                                pin_memory=(torch.cuda.is_available()), num_workers=cfg.num_workers)\n",
    "    test_loader = DataLoader(test_set, batch_size=cfg.test_batch_size, shuffle=False,\n",
    "                                pin_memory=(torch.cuda.is_available()), num_workers=cfg.num_workers)\n",
    "    if valid_set is None:\n",
    "        valid_loader = None\n",
    "    else:\n",
    "        valid_loader = DataLoader(valid_set, batch_size=cfg.batch_size, shuffle=False,\n",
    "                                pin_memory=(torch.cuda.is_available()), num_workers=cfg.num_workers)\n",
    "    # Model on cuda\n",
    "    model = to_device(model)\n",
    "\n",
    "    # Wrap model for multi-GPUs, if necessary\n",
    "    model_wrapper = model\n",
    "    if torch.cuda.is_available() and torch.cuda.device_count() > 1:\n",
    "        model_wrapper = torch.nn.DataParallel(model).cuda()\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.Adam(model_wrapper.parameters(), lr=cfg.lr)\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=cfg.milestones,\n",
    "                                                     gamma=cfg.gamma)\n",
    "\n",
    "    # Start log\n",
    "    logs = ['loss', 'iou', 'dice'] + ['iou{}'.format(i) for i in range(4)]+['dice{}'.format(i) for i in range(4)]\n",
    "    train_logs = ['train_'+log for log in logs]\n",
    "    test_logs = ['test_'+log for log in logs]\n",
    "    log_dict = OrderedDict.fromkeys(train_logs+test_logs, 0)\n",
    "    with open(os.path.join(save, 'logs.csv'), 'w') as f:\n",
    "        f.write('epoch,')\n",
    "        for key in log_dict.keys():\n",
    "            f.write(key+',')\n",
    "        f.write('\\n')\n",
    "    writer = SummaryWriter(log_dir=os.path.join(save, 'Tensorboard_Results'))\n",
    "\n",
    "    # Train model\n",
    "    best_dice = 0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        os.makedirs(os.path.join(cfg.save, 'epoch_{}'.format(epoch)),exist_ok = True)\n",
    "        train_meters = train_epoch(\n",
    "            model=model_wrapper,\n",
    "            loader=train_loader,\n",
    "            optimizer=optimizer,\n",
    "            epoch=epoch,\n",
    "            n_epochs=n_epochs,\n",
    "            writer=writer\n",
    "        )\n",
    "        # if (epoch+1)%5==0:\n",
    "        test_meters = test_epoch(\n",
    "            model=model_wrapper,\n",
    "            loader=test_loader,\n",
    "            epoch=epoch,\n",
    "            is_test=True,\n",
    "            writer = writer\n",
    "        )\n",
    "        scheduler.step()\n",
    "\n",
    "        # Log results\n",
    "        for i, key in enumerate(train_logs):\n",
    "            log_dict[key] = train_meters[i]\n",
    "        for i, key in enumerate(test_logs):\n",
    "            log_dict[key] = test_meters[i]\n",
    "\n",
    "        log_results(save, epoch, log_dict, writer=writer)\n",
    "\n",
    "        if cfg.save_all:\n",
    "            torch.save(model.state_dict(), os.path.join(save, 'epoch_{}'.format(epoch), 'model.dat'))\n",
    "\n",
    "        if log_dict['test_dice'] > best_dice:\n",
    "            torch.save(model.state_dict(), os.path.join(save, 'model.dat'))\n",
    "            best_dice = log_dict['test_dice']\n",
    "            print('New best dice: %.4f' % log_dict['test_dice'])\n",
    "            #print(2.*intersection/union)\n",
    "        else:\n",
    "            print('Current best dice: %.4f' % best_dice)\n",
    "            #print(2.*intersection/union)\n",
    "    writer.close()\n",
    "\n",
    "    with open(os.path.join(save, 'logs.csv'), 'a') as f:\n",
    "        f.write(',,,,best dice,%0.5f\\n' % (best_dice))\n",
    "    # Final test of the best model on test set\n",
    "    print('best dice: ', best_dice)\n",
    "\n",
    "iteration = 0\n",
    "\n",
    "def train_epoch(model, loader, optimizer, epoch, n_epochs, print_freq=1, writer=None):\n",
    "    meters = MultiAverageMeter()\n",
    "    # Model on train mode\n",
    "    model.train()\n",
    "    global iteration\n",
    "    end = time.time()\n",
    "    for batch_idx, (x, y) in enumerate(loader):\n",
    "        # Create vaiables\n",
    "        x = to_var(x)\n",
    "        y = to_var(y)\n",
    "        # compute output\n",
    "        pred_logit = model(x)\n",
    "        loss = soft_dice_loss(pred_logit, y, smooth=1e-2)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        y = y.long()\n",
    "        \n",
    "        batch_size = y.size(0)\n",
    "        iou = cal_batch_iou(pred_logit, y)\n",
    "        dice = cal_batch_dice(pred_logit, y)\n",
    "\n",
    "        logs = [loss.item(), iou[1:].mean(), dice[1:].mean()]+ \\\n",
    "                            [iou[i].item() for i in range(len(iou))]+ \\\n",
    "                            [dice[i].item() for i in range(len(dice))]+ \\\n",
    "                            [time.time() - end]\n",
    "        meters.update(logs, batch_size)   \n",
    "        writer.add_scalar('train_loss_logs', loss.item(), iteration)\n",
    "        with open(os.path.join(cfg.save, 'loss_logs.csv'), 'a') as f:\n",
    "            f.write('%09d,%0.6f,\\n'%((iteration + 1),loss.item(),))\n",
    "        iteration += 1\n",
    "\n",
    "\n",
    "        # measure elapsed time\n",
    "        end = time.time()\n",
    "        # print stats\n",
    "        print_freq = 2 // meters.val[-1] + 1\n",
    "        if batch_idx % print_freq == 0:\n",
    "            res = '\\t'.join([\n",
    "                'Epoch: [%d/%d]' % (epoch + 1, n_epochs),\n",
    "                'Iter: [%d/%d]' % (batch_idx + 1, len(loader)),\n",
    "                'Time %.3f (%.3f)' % (meters.val[-1], meters.avg[-1]),\n",
    "                'Loss %.4f (%.4f)' % (meters.val[0], meters.avg[0]),\n",
    "                'IOU %.4f (%.4f)' % (meters.val[1], meters.avg[1]),\n",
    "                'DICE %.4f (%.4f)' % (meters.val[2], meters.avg[2]),\n",
    "            ])\n",
    "            print(res)\n",
    "\n",
    "    return meters.avg[:-1] #intersection, union\n",
    "\n",
    "\n",
    "def test_epoch(model, loader, epoch, print_freq=1, is_test=True, writer=None):\n",
    "    meters = MultiAverageMeter()\n",
    "    # Model on eval mode\n",
    "    model.eval()\n",
    "    gt_classes = []\n",
    "    pred_all_probs = []\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (x, y) in enumerate(loader):\n",
    "            \n",
    "            x = to_var(x)\n",
    "            \n",
    "            \n",
    "            y = to_var(y)\n",
    "            \n",
    "            pred_logit = model(x)\n",
    "            \n",
    "            # calculate metrics\n",
    "            pred_class = pred_logit.max(dim=1)[1]\n",
    "            pred_probs = pred_logit.softmax(-1)\n",
    "            pred_all_probs.append(pred_probs.cpu())\n",
    "            gt_classes.append(y.cpu())\n",
    "            \n",
    "            #print(gt_classes.shape) #pred_class[20,48,48,48]\n",
    "            #print(pred_probs[1]) #y e pred_probs[20,6,48,48,48]\n",
    "            \n",
    "            batch_size, n_classes = pred_logit.shape[:2]\n",
    "            \n",
    "\n",
    "            loss = soft_dice_loss(pred_logit, y, smooth=1e-2)\n",
    "            y = y.long()\n",
    "            batch_size = y.size(0)\n",
    "            iou = cal_batch_iou(pred_logit, y)\n",
    "            dice = cal_batch_dice(pred_logit, y)\n",
    " \n",
    "            logs = [loss.item(), iou[1:].mean(), dice[1:].mean()]+ \\\n",
    "                                [iou[i].item() for i in range(len(iou))]+ \\\n",
    "                                [dice[i].item() for i in range(len(dice))]+ \\\n",
    "                                [time.time() - end]\n",
    "            meters.update(logs, batch_size)\n",
    "\n",
    "            end = time.time()\n",
    "\n",
    "            print_freq = 2 // meters.val[-1] + 1\n",
    "            if batch_idx % print_freq == 0:\n",
    "                res = '\\t'.join([\n",
    "                    'Test' if is_test else 'Valid',\n",
    "                    'Iter: [%d/%d]' % (batch_idx + 1, len(loader)),\n",
    "                    'Time %.3f (%.3f)' % (meters.val[-1], meters.avg[-1]),\n",
    "                    'Loss %.4f (%.4f)' % (meters.val[0], meters.avg[0]),\n",
    "                    'IOU %.4f (%.4f)' % (meters.val[1], meters.avg[1]),\n",
    "                    'DICE %.4f (%.4f)' % (meters.val[2], meters.avg[2]),\n",
    "                ])\n",
    "                print(res)\n",
    "\n",
    "    return meters.avg[:-1]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    fire.Fire(main)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding: utf-8\n",
    "#Conv 2,5d Random\n",
    "import _init_paths\n",
    "\n",
    "import fire\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import sklearn\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from poc_dataset_ACS import BaseDatasetVoxel\n",
    "from mylib.loss import soft_cross_entropy_loss\n",
    "from mylib.utils import MultiAverageMeter, save_model, log_results, to_var, set_seed, \\\n",
    "        to_device, initialize, categorical_to_one_hot, copy_file_backup, redirect_stdout\n",
    "from poc_config_2drand import POCVoxelConfig as cfg\n",
    "from poc_config_2drand import POCVoxelEnv as env\n",
    "\n",
    "from unet import UNet\n",
    "from acsconv.models import ACSUNet\n",
    "from acsconv.converters import ACSConverter, Conv3dConverter, Conv2_5dConverter\n",
    "\n",
    "from mylib.metrics import cal_batch_iou, cal_batch_dice\n",
    "from mylib.loss import soft_dice_loss\n",
    "import timeit\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "\n",
    "def main(save_path=cfg.save, \n",
    "         n_epochs=cfg.n_epochs, \n",
    "         seed=cfg.seed\n",
    "         ):\n",
    "    if seed is not None:\n",
    "        set_seed(cfg.seed)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "\n",
    "\n",
    "    # # Models\n",
    "    os.makedirs(save_path,exist_ok = True)\n",
    "    #copy_file_backup(save_path)\n",
    "    redirect_stdout(save_path)\n",
    "\n",
    "    # Datasets\n",
    "    train_data = env.data_train\n",
    "    test_data = env.data_train\n",
    "    shape_cp = env.shape_checkpoint\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    train_set = BaseDatasetVoxel(train_data, cfg.train_samples)\n",
    "    valid_set = None\n",
    "    test_set = BaseDatasetVoxel(test_data, cfg.train_samples)\n",
    "\n",
    "    model = UNet(4)\n",
    "    if cfg.conv == 'Conv3D':\n",
    "        model = Conv3dConverter(model)\n",
    "        initialize(model.modules())\n",
    "    elif cfg.conv == 'Conv2_5D':\n",
    "        print('REDE 2,5D')\n",
    "        if cfg.pretrained:\n",
    "            shape_cp = torch.load(shape_cp)\n",
    "            shape_cp.popitem()\n",
    "            shape_cp.popitem()\n",
    "            incompatible_keys = model.load_state_dict(shape_cp, strict=False)\n",
    "            print('load shape pretrained weights\\n', incompatible_keys)\n",
    "        model = Conv2_5dConverter(model)\n",
    "    elif cfg.conv == 'ACSConv':\n",
    "        # You can use either the naive ``ACSUNet`` or the ``ACSConverter(model)``\n",
    "        #model = ACSConverter(model)\n",
    "        model = ACSUNet(4)\n",
    "        if cfg.pretrained:\n",
    "            shape_cp = torch.load(shape_cp)\n",
    "            shape_cp.popitem()\n",
    "            shape_cp.popitem()\n",
    "            incompatible_keys = model.load_state_dict(shape_cp, strict=False)\n",
    "            print('load shape pretrained weights\\n', incompatible_keys)\n",
    "    else:\n",
    "        raise ValueError('not valid conv')\n",
    "    \n",
    "    print(model)\n",
    "    torch.save(model.state_dict(), os.path.join(save_path, 'model.dat'))\n",
    "    # Train the model\n",
    "    train(model=model, train_set=train_set, valid_set=valid_set, test_set=test_set, save=save_path, n_epochs=n_epochs)\n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "    print(elapsed)\n",
    "    print('Done!')\n",
    "\n",
    "\n",
    "\n",
    "def train(model, train_set, test_set, save, valid_set, n_epochs):\n",
    "\n",
    "\n",
    "    # Data loaders\n",
    "    train_loader = DataLoader(train_set, batch_size=cfg.train_batch_size, shuffle=True,\n",
    "                                pin_memory=(torch.cuda.is_available()), num_workers=cfg.num_workers)\n",
    "    test_loader = DataLoader(test_set, batch_size=cfg.test_batch_size, shuffle=False,\n",
    "                                pin_memory=(torch.cuda.is_available()), num_workers=cfg.num_workers)\n",
    "    if valid_set is None:\n",
    "        valid_loader = None\n",
    "    else:\n",
    "        valid_loader = DataLoader(valid_set, batch_size=cfg.batch_size, shuffle=False,\n",
    "                                pin_memory=(torch.cuda.is_available()), num_workers=cfg.num_workers)\n",
    "    # Model on cuda\n",
    "    model = to_device(model)\n",
    "\n",
    "    # Wrap model for multi-GPUs, if necessary\n",
    "    model_wrapper = model\n",
    "    if torch.cuda.is_available() and torch.cuda.device_count() > 1:\n",
    "        model_wrapper = torch.nn.DataParallel(model).cuda()\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.Adam(model_wrapper.parameters(), lr=cfg.lr)\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=cfg.milestones,\n",
    "                                                     gamma=cfg.gamma)\n",
    "\n",
    "    # Start log\n",
    "    logs = ['loss', 'iou', 'dice'] + ['iou{}'.format(i) for i in range(4)]+['dice{}'.format(i) for i in range(4)]\n",
    "    train_logs = ['train_'+log for log in logs]\n",
    "    test_logs = ['test_'+log for log in logs]\n",
    "    log_dict = OrderedDict.fromkeys(train_logs+test_logs, 0)\n",
    "    with open(os.path.join(save, 'logs.csv'), 'w') as f:\n",
    "        f.write('epoch,')\n",
    "        for key in log_dict.keys():\n",
    "            f.write(key+',')\n",
    "        f.write('\\n')\n",
    "    writer = SummaryWriter(log_dir=os.path.join(save, 'Tensorboard_Results'))\n",
    "\n",
    "    # Train model\n",
    "    best_dice = 0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        os.makedirs(os.path.join(cfg.save, 'epoch_{}'.format(epoch)),exist_ok = True)\n",
    "        train_meters = train_epoch(\n",
    "            model=model_wrapper,\n",
    "            loader=train_loader,\n",
    "            optimizer=optimizer,\n",
    "            epoch=epoch,\n",
    "            n_epochs=n_epochs,\n",
    "            writer=writer\n",
    "        )\n",
    "        # if (epoch+1)%5==0:\n",
    "        test_meters = test_epoch(\n",
    "            model=model_wrapper,\n",
    "            loader=test_loader,\n",
    "            epoch=epoch,\n",
    "            is_test=True,\n",
    "            writer = writer\n",
    "        )\n",
    "        scheduler.step()\n",
    "\n",
    "        # Log results\n",
    "        for i, key in enumerate(train_logs):\n",
    "            log_dict[key] = train_meters[i]\n",
    "        for i, key in enumerate(test_logs):\n",
    "            log_dict[key] = test_meters[i]\n",
    "\n",
    "        log_results(save, epoch, log_dict, writer=writer)\n",
    "\n",
    "        if cfg.save_all:\n",
    "            torch.save(model.state_dict(), os.path.join(save, 'epoch_{}'.format(epoch), 'model.dat'))\n",
    "\n",
    "        if log_dict['test_dice'] > best_dice:\n",
    "            torch.save(model.state_dict(), os.path.join(save, 'model.dat'))\n",
    "            best_dice = log_dict['test_dice']\n",
    "            print('New best dice: %.4f' % log_dict['test_dice'])\n",
    "            #print(2.*intersection/union)\n",
    "        else:\n",
    "            print('Current best dice: %.4f' % best_dice)\n",
    "            #print(2.*intersection/union)\n",
    "    writer.close()\n",
    "\n",
    "    with open(os.path.join(save, 'logs.csv'), 'a') as f:\n",
    "        f.write(',,,,best dice,%0.5f\\n' % (best_dice))\n",
    "    # Final test of the best model on test set\n",
    "    print('best dice: ', best_dice)\n",
    "\n",
    "iteration = 0\n",
    "\n",
    "def train_epoch(model, loader, optimizer, epoch, n_epochs, print_freq=1, writer=None):\n",
    "    meters = MultiAverageMeter()\n",
    "    # Model on train mode\n",
    "    model.train()\n",
    "    global iteration\n",
    "    end = time.time()\n",
    "    for batch_idx, (x, y) in enumerate(loader):\n",
    "        # Create vaiables\n",
    "        x = to_var(x)\n",
    "        y = to_var(y)\n",
    "        # compute output\n",
    "        pred_logit = model(x)\n",
    "        loss = soft_dice_loss(pred_logit, y, smooth=1e-2)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        y = y.long()\n",
    "        \n",
    "        batch_size = y.size(0)\n",
    "        iou = cal_batch_iou(pred_logit, y)\n",
    "        dice = cal_batch_dice(pred_logit, y)\n",
    "\n",
    "        logs = [loss.item(), iou[1:].mean(), dice[1:].mean()]+ \\\n",
    "                            [iou[i].item() for i in range(len(iou))]+ \\\n",
    "                            [dice[i].item() for i in range(len(dice))]+ \\\n",
    "                            [time.time() - end]\n",
    "        meters.update(logs, batch_size)   \n",
    "        writer.add_scalar('train_loss_logs', loss.item(), iteration)\n",
    "        with open(os.path.join(cfg.save, 'loss_logs.csv'), 'a') as f:\n",
    "            f.write('%09d,%0.6f,\\n'%((iteration + 1),loss.item(),))\n",
    "        iteration += 1\n",
    "\n",
    "\n",
    "        # measure elapsed time\n",
    "        end = time.time()\n",
    "        # print stats\n",
    "        print_freq = 2 // meters.val[-1] + 1\n",
    "        if batch_idx % print_freq == 0:\n",
    "            res = '\\t'.join([\n",
    "                'Epoch: [%d/%d]' % (epoch + 1, n_epochs),\n",
    "                'Iter: [%d/%d]' % (batch_idx + 1, len(loader)),\n",
    "                'Time %.3f (%.3f)' % (meters.val[-1], meters.avg[-1]),\n",
    "                'Loss %.4f (%.4f)' % (meters.val[0], meters.avg[0]),\n",
    "                'IOU %.4f (%.4f)' % (meters.val[1], meters.avg[1]),\n",
    "                'DICE %.4f (%.4f)' % (meters.val[2], meters.avg[2]),\n",
    "            ])\n",
    "            print(res)\n",
    "\n",
    "    return meters.avg[:-1] #intersection, union\n",
    "\n",
    "\n",
    "def test_epoch(model, loader, epoch, print_freq=1, is_test=True, writer=None):\n",
    "    meters = MultiAverageMeter()\n",
    "    # Model on eval mode\n",
    "    model.eval()\n",
    "    gt_classes = []\n",
    "    pred_all_probs = []\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (x, y) in enumerate(loader):\n",
    "            \n",
    "            x = to_var(x)\n",
    "            \n",
    "            \n",
    "            y = to_var(y)\n",
    "            \n",
    "            pred_logit = model(x)\n",
    "            \n",
    "            # calculate metrics\n",
    "            pred_class = pred_logit.max(dim=1)[1]\n",
    "            pred_probs = pred_logit.softmax(-1)\n",
    "            pred_all_probs.append(pred_probs.cpu())\n",
    "            gt_classes.append(y.cpu())\n",
    "            \n",
    "            #print(gt_classes.shape) #pred_class[20,48,48,48]\n",
    "            #print(pred_probs[1]) #y e pred_probs[20,6,48,48,48]\n",
    "            \n",
    "            batch_size, n_classes = pred_logit.shape[:2]\n",
    "            \n",
    "\n",
    "            loss = soft_dice_loss(pred_logit, y, smooth=1e-2)\n",
    "            y = y.long()\n",
    "            batch_size = y.size(0)\n",
    "            iou = cal_batch_iou(pred_logit, y)\n",
    "            dice = cal_batch_dice(pred_logit, y)\n",
    " \n",
    "            logs = [loss.item(), iou[1:].mean(), dice[1:].mean()]+ \\\n",
    "                                [iou[i].item() for i in range(len(iou))]+ \\\n",
    "                                [dice[i].item() for i in range(len(dice))]+ \\\n",
    "                                [time.time() - end]\n",
    "            meters.update(logs, batch_size)\n",
    "\n",
    "            end = time.time()\n",
    "\n",
    "            print_freq = 2 // meters.val[-1] + 1\n",
    "            if batch_idx % print_freq == 0:\n",
    "                res = '\\t'.join([\n",
    "                    'Test' if is_test else 'Valid',\n",
    "                    'Iter: [%d/%d]' % (batch_idx + 1, len(loader)),\n",
    "                    'Time %.3f (%.3f)' % (meters.val[-1], meters.avg[-1]),\n",
    "                    'Loss %.4f (%.4f)' % (meters.val[0], meters.avg[0]),\n",
    "                    'IOU %.4f (%.4f)' % (meters.val[1], meters.avg[1]),\n",
    "                    'DICE %.4f (%.4f)' % (meters.val[2], meters.avg[2]),\n",
    "                ])\n",
    "                print(res)\n",
    "\n",
    "    return meters.avg[:-1]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    fire.Fire(main)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "x= pd.read_csv(r'C:\\Users\\admin\\Desktop\\cnn-facies-classifier-master\\tmp\\voxel\\Conv3D_330samples\\logs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import pandas as pd\n",
    "a=sys.path[1]\n",
    "x= pd.read_csv(a+r'\\RESULTADOS_EQM\\CHANNEL1\\Conv2_5D_2640samples\\pretrained\\logs.csv')\n",
    "\n",
    "#df= x[['train_loss', 'train_dice']]\n",
    "loss=x['train_loss']\n",
    "testdice=x['test_dice']\n",
    "dice=x['train_dice']\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(loss, color='red', label='train_loss')\n",
    "plt.legend()\n",
    "plt.ylim(min(loss)*0.90, max(loss)*1.05)\n",
    "plt.xlim(0, len(loss))\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epochs')\n",
    " \n",
    "plt.title(\"Log loss\", fontsize=22)\n",
    "plt.grid(axis='both', alpha=.3)\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(dice, color='blue', label='train_dice')\n",
    "\n",
    "#plt.plot(testdice, color='green', label='test_dice')\n",
    "plt.legend()\n",
    "plt.ylim(min(dice)*0.9, 1)\n",
    "plt.xlim(0, len(dice))\n",
    "plt.ylabel('Dice/F1-Score')\n",
    "plt.xlabel('epochs')\n",
    "\n",
    "plt.subplots_adjust(wspace=0.5, hspace=0.5)\n",
    "plt.title(\"Dice/F1-Score\", fontsize=22)\n",
    "plt.grid(axis='both', alpha=.3)\n",
    "\n",
    "#axes = df.plot.line(subplots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import pandas as pd\n",
    "import sys\n",
    "a=sys.path[1]\n",
    "UNET3D= pd.read_csv(a+r'\\RESULTADOS_EQM\\CHANNEL0\\Conv3D_330samples\\logs.csv')\n",
    "ACSUNETP=pd.read_csv(a+r'\\RESULTADOS_EQM\\CHANNEL0\\ACSConv_1320samples\\PRETRAINED\\logs.csv')\n",
    "ACSUNETR=pd.read_csv(a+r'\\RESULTADOS_EQM\\CHANNEL0\\ACSConv_1320samples\\RANDOM\\logs.csv')\n",
    "BIDIM=pd.read_csv(a+r'\\RESULTADOS_EQM\\CHANNEL0\\Conv2_5D_2640samples\\PRETRAINED\\logs.csv')\n",
    "#df= x[['train_loss', 'train_dice']]\n",
    "loss=[UNET3D['train_loss'],ACSUNETP['train_loss'], ACSUNETR['train_loss'],BIDIM['train_loss']]\n",
    "testdice=[UNET3D['test_dice'],ACSUNETP['test_dice'], ACSUNETR['test_dice'],BIDIM['test_dice']]\n",
    "dice=[UNET3D['train_dice'],ACSUNETP['train_dice'], ACSUNETR['train_dice'],BIDIM['train_dice']]\n",
    "\n",
    "plt.figure(figsize=(10,20))\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(loss[0], color='red', label='train_loss_UNET3D')\n",
    "plt.plot(loss[1], color='green', label='train_loss_ACSUNETP')\n",
    "plt.plot(loss[2], color='blue', label='train_loss_ACSUNETR')\n",
    "plt.plot(loss[3], color='purple', label='train_loss_2,5D')\n",
    "plt.legend()\n",
    "#plt.ylim(min(loss[0].all)*0.90, max(loss)*1.05)\n",
    "plt.xlim(0, len(loss[0]))\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epochs')\n",
    " \n",
    "plt.title(\"Log loss\", fontsize=22)\n",
    "plt.grid(axis='both', alpha=.3)\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(dice[0], color='red', label='train_dice_UNET3D')\n",
    "plt.plot(dice[1], color='green', label='train_dice_ACSUNETP')\n",
    "plt.plot(dice[2], color='blue', label='train_dice_ACSUNETR')\n",
    "plt.plot(dice[3], color='purple', label='train_dice_2,5D')\n",
    "\n",
    "\n",
    "#plt.plot(testdice, color='green', label='test_dice')\n",
    "plt.legend()\n",
    "#plt.ylim(min(dice)*0.9, 1)\n",
    "plt.xlim(0, len(dice[0]))\n",
    "plt.ylabel('Dice/F1-Score')\n",
    "plt.xlabel('epochs')\n",
    "\n",
    "plt.subplots_adjust(wspace=0.5, hspace=0.5)\n",
    "plt.title(\"Dice/F1-Score\", fontsize=22)\n",
    "plt.grid(axis='both', alpha=.3)\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(testdice[0], color='red', label='test_dice_UNET3D')\n",
    "plt.plot(testdice[1], color='green', label='test_dice_ACSUNETP')\n",
    "plt.plot(testdice[2], color='blue', label='test_dice_ACSUNETR')\n",
    "plt.plot(testdice[3], color='purple', label='test_dice_2,5D')\n",
    "\n",
    "\n",
    "#plt.plot(testdice, color='green', label='test_dice')\n",
    "plt.legend()\n",
    "plt.ylim(min(testdice[3])*0.9, 0.9)\n",
    "plt.xlim(0, len(testdice[0]))\n",
    "plt.ylabel('Dice/F1-Score')\n",
    "plt.xlabel('epochs')\n",
    "\n",
    "plt.subplots_adjust(wspace=0.5, hspace=0.5)\n",
    "plt.title(\"Dice/F1-Score\", fontsize=22)\n",
    "plt.grid(axis='both', alpha=.3)\n",
    "\n",
    "#axes = df.plot.line(subplots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import pandas as pd\n",
    "import sys\n",
    "a=sys.path[1]\n",
    "ACSUNETP0= pd.read_csv(a+r'\\RESULTADOS_EQM\\CHANNEL0\\ACSConv_1320samples\\PRETRAINED\\logs.csv')\n",
    "ACSUNETP1=pd.read_csv(a+r'\\RESULTADOS_EQM\\CHANNEL1\\ACSConv_1320samples\\PRETRAINED\\logs.csv')\n",
    "ACSUNETP2=pd.read_csv(a+r'\\RESULTADOS_EQM\\CHANNEL2\\ACSConv_1320samples\\PRETRAINED\\logs.csv')\n",
    "\n",
    "#df= x[['train_loss', 'train_dice']]\n",
    "loss=[ACSUNETP0['train_loss'],ACSUNETP1['train_loss'], ACSUNETP2['train_loss']]\n",
    "testdice=[ACSUNETP0['test_dice'],ACSUNETP1['test_dice'], ACSUNETP2['test_dice']]\n",
    "dice=[ACSUNETP0['train_dice'],ACSUNETP1['train_dice'], ACSUNETP2['train_dice']]\n",
    "\n",
    "plt.figure(figsize=(10,20))\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(loss[0], color='red', label='Channel0')\n",
    "plt.plot(loss[1], color='green', label='Channel1')\n",
    "plt.plot(loss[2], color='blue', label='Channel2')\n",
    "\n",
    "plt.legend()\n",
    "#plt.ylim(min(loss[0].all)*0.90, max(loss)*1.05)\n",
    "plt.xlim(0, len(loss[0]))\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epochs')\n",
    " \n",
    "plt.title(\"Log loss\", fontsize=22)\n",
    "plt.grid(axis='both', alpha=.3)\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(dice[0], color='red', label='Channel0')\n",
    "plt.plot(dice[1], color='green', label='Channel1')\n",
    "plt.plot(dice[2], color='blue', label='Channel2')\n",
    "\n",
    "\n",
    "#plt.plot(testdice, color='green', label='test_dice')\n",
    "plt.legend()\n",
    "#plt.ylim(min(dice)*0.9, 1)\n",
    "plt.xlim(0, len(dice[0]))\n",
    "plt.ylabel('Dice/F1-Score')\n",
    "plt.xlabel('epochs')\n",
    "\n",
    "plt.subplots_adjust(wspace=0.5, hspace=0.5)\n",
    "plt.title(\"Dice/F1-Score\", fontsize=22)\n",
    "plt.grid(axis='both', alpha=.3)\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(testdice[0], color='red', label='Channel0')\n",
    "plt.plot(testdice[1], color='green', label='Channel1')\n",
    "plt.plot(testdice[2], color='blue', label='Channel2')\n",
    "\n",
    "\n",
    "#plt.plot(testdice, color='green', label='test_dice')\n",
    "plt.legend()\n",
    "plt.ylim(min(testdice[2])*0.9, 0.9)\n",
    "plt.xlim(0, len(testdice[0]))\n",
    "plt.ylabel('Dice/F1-Score')\n",
    "plt.xlabel('epochs')\n",
    "\n",
    "plt.subplots_adjust(wspace=0.5, hspace=0.5)\n",
    "plt.title(\"Dice/F1-Score\", fontsize=22)\n",
    "plt.grid(axis='both', alpha=.3)\n",
    "\n",
    "#axes = df.plot.line(subplots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
